[
  {
    "id": "ace-compute-001",
    "domain": "compute",
    "difficulty": "easy",
    "type": "multiple-choice",
    "scenario": "Your team is migrating a small line-of-business application from an on-premises physical server to Google Cloud. The application needs full control over the operating system, custom startup scripts, and uses about 200 GB of persistent storage. You want to lift and shift the workload with minimal code changes and be able to scale up to a few dozen instances later.",
    "question": "Which GCP service should you use?",
    "options": [
      {
        "id": "A",
        "text": "Cloud TPU to accelerate the migrated application using matrix multiplication hardware"
      },
      {
        "id": "B",
        "text": "Compute Engine virtual machines to run the application on managed VMs with full OS control"
      },
      {
        "id": "C",
        "text": "App Engine standard environment to run the application without managing any servers"
      },
      {
        "id": "D",
        "text": "Cloud Functions (2nd gen) to host the entire application as event-driven functions"
      }
    ],
    "correctAnswer": ["B"],
    "explanation": {
      "correct": "Compute Engine VMs are the Google-recommended choice for lift-and-shift workloads that need full OS control, custom startup scripts, and persistent disks without requiring code changes. This aligns with the decision criteria that arbitrary applications and traditional server workloads should run on Compute Engine rather than serverless or specialized hardware.",
      "incorrect": {
        "A": "Cloud TPUs are designed for large-scale machine learning workloads that use matrix multiplication and train for long durations, not generic line-of-business applications. Using TPUs here would overcomplicate the solution and does not match the workload type.",
        "C": "App Engine standard requires adapting the application to specific runtimes and deployment models, which conflicts with the requirement for minimal code changes and full OS control.",
        "D": "Cloud Functions (2nd gen) is optimized for event-driven, short-lived functions with request-based execution rather than a full server application that needs persistent storage and full OS customization."
      }
    },
    "keyConceptName": "Service Selection – Lift-and-Shift to Compute Engine",
    "keyConcept": "When migrating traditional server applications that require full OS control and minimal code changes, Google recommends using Compute Engine virtual machines rather than serverless or specialized hardware services.",
    "tags": [
      "service-selection",
      "compute-engine",
      "lift-and-shift",
      "vm-migration",
      "os-control"
    ],
    "examPatternKeywords": [
      "Which GCP service should you use?",
      "lift and shift",
      "virtual machines",
      "service selection"
    ],
    "relatedQuestionIds": ["ace-compute-002", "ace-compute-006"],
    "officialDocsUrl": "https://cloud.google.com/compute/docs/instances"
  },
  {
    "id": "ace-compute-002",
    "domain": "compute",
    "difficulty": "easy",
    "type": "multiple-choice",
    "scenario": "A team wants to deploy a new internal business application that needs to run 24x7, supports around 500 users, and requires custom OS packages and startup scripts. They expect to scale from 1 to 20 instances over the next 12 months and want to retain control over machine types, disks, and network settings.",
    "question": "Which GCP service should you use?",
    "options": [
      {
        "id": "A",
        "text": "Cloud TPU with custom containers running the application logic"
      },
      {
        "id": "B",
        "text": "Compute Engine virtual machines with managed instance groups for scaling"
      },
      {
        "id": "C",
        "text": "BigQuery with user-defined functions to host the application logic"
      },
      {
        "id": "D",
        "text": "Cloud Functions (1st gen) with a 9-minute timeout for each request"
      }
    ],
    "correctAnswer": ["B"],
    "explanation": {
      "correct": "Compute Engine VMs, optionally using managed instance groups, are recommended for 24x7 applications that need full OS control, custom packages, and predictable vertical and horizontal scaling. This matches the decision criteria for arbitrary server workloads with user-controlled machine types and disks.",
      "incorrect": {
        "A": "Cloud TPUs are specialized accelerators for large-scale machine learning and do not provide the general-purpose OS environment needed to host a 24x7 business application.",
        "C": "BigQuery is a data warehouse for analytics, not an application hosting platform, and cannot replace a general-purpose compute environment.",
        "D": "Cloud Functions (1st gen) has a 9-minute timeout and is designed for short-lived event-driven functions, not always-on application servers for 500 users."
      }
    },
    "keyConceptName": "Service Selection – Always-On Business Apps on VMs",
    "keyConcept": "For always-on business applications that require full OS control, predictable scaling, and customization, Compute Engine virtual machines are the recommended service.",
    "tags": [
      "compute-engine",
      "managed-instance-groups",
      "service-selection",
      "always-on-workloads",
      "scaling"
    ],
    "examPatternKeywords": [
      "Which GCP service should you use?",
      "24x7 application",
      "virtual machines",
      "custom OS"
    ],
    "relatedQuestionIds": ["ace-compute-001", "ace-compute-005"],
    "officialDocsUrl": "https://cloud.google.com/compute/docs/autoscaler"
  },
  {
    "id": "ace-compute-003",
    "domain": "compute",
    "difficulty": "medium",
    "type": "multiple-choice",
    "scenario": "Your data science team is training deep learning models that run for 18–20 hours per experiment using large effective batch sizes and matrices. They need maximum training throughput and energy efficiency for large-scale linear algebra operations executed repeatedly during training. The team is comfortable adapting their code to run on specialized accelerators.",
    "question": "Which GCP service should you use?",
    "options": [
      {
        "id": "A",
        "text": "Compute Engine VMs with only standard CPUs and no accelerators"
      },
      {
        "id": "B",
        "text": "Cloud TPU to run the training workloads on dedicated tensor processing units"
      },
      {
        "id": "C",
        "text": "Cloud Functions (2nd gen) with each function handling a batch of matrix operations"
      },
      {
        "id": "D",
        "text": "App Engine standard environment with CPU autoscaling"
      }
    ],
    "correctAnswer": ["B"],
    "explanation": {
      "correct": "Cloud TPUs are specifically designed for large-scale machine learning with long-running training jobs, large effective batch sizes, and heavy matrix multiplication. Google recommends TPUs when you need maximum training throughput and energy efficiency for deep learning workloads.",
      "incorrect": {
        "A": "CPU-only VMs can train models but will be significantly slower and less energy-efficient than TPUs for large-scale deep learning, making this a suboptimal choice given the performance requirement.",
        "C": "Cloud Functions are not suited for 18–20 hour training jobs due to their request-driven model and timeouts; they are intended for short-lived event processing, not continuous ML training.",
        "D": "App Engine standard abstracts the underlying hardware and is not designed to provide direct access to TPUs or optimized matrix multiplication for deep learning training."
      }
    },
    "keyConceptName": "Service Selection – ML Training on Cloud TPUs",
    "keyConcept": "When training large deep learning models for many hours with heavy matrix operations and large batch sizes, Cloud TPUs are the recommended Google Cloud service for maximum performance and efficiency.",
    "tags": [
      "cloud-tpu",
      "machine-learning",
      "service-selection",
      "training-throughput",
      "accelerators"
    ],
    "examPatternKeywords": [
      "Which GCP service should you use?",
      "machine learning",
      "training",
      "TPU"
    ],
    "relatedQuestionIds": ["ace-compute-004", "ace-compute-007"],
    "officialDocsUrl": "https://cloud.google.com/tpu/docs/overview"
  },
  {
    "id": "ace-compute-004",
    "domain": "compute",
    "difficulty": "medium",
    "type": "multiple-choice",
    "scenario": "A research lab runs hundreds of nightly ML training jobs that each take around 9–10 hours to complete. The jobs are highly parallel, use large matrix multiplications, and must finish within a 24-hour window so that new models are ready every morning. The team wants to minimize wall-clock training time for these 10-hour runs.",
    "question": "Which GCP service should you use?",
    "options": [
      {
        "id": "A",
        "text": "Compute Engine VMs with general-purpose machine types and no accelerators"
      },
      {
        "id": "B",
        "text": "Cloud TPU to accelerate large-batch, 10-hour ML training jobs"
      },
      {
        "id": "C",
        "text": "Cloud Functions (1st gen) chained together, each limited to a 9-minute timeout"
      },
      {
        "id": "D",
        "text": "Cloud Storage lifecycle rules to optimize training performance"
      }
    ],
    "correctAnswer": ["B"],
    "explanation": {
      "correct": "Cloud TPUs are optimized for high-throughput, long-running ML training with repeated matrix operations, making them ideal for nightly 9–10 hour training jobs that must complete within a 24-hour window. Google recommends TPUs for such large-scale training workloads.",
      "incorrect": {
        "A": "CPU-only general-purpose VMs can run ML code but will be significantly slower for large matrix operations, making it harder to meet the daily 24-hour turnaround.",
        "C": "Cloud Functions (1st gen) have a 9-minute timeout limit and are not suited to 9–10 hour continuous training jobs, even if chained together, which would be an overcomplicated and fragile design.",
        "D": "Cloud Storage lifecycle rules manage object storage classes and retention; they do not provide compute capabilities or improve training throughput."
      }
    },
    "keyConceptName": "Service Selection – Long-Running ML Jobs on TPUs",
    "keyConcept": "For 9–10 hour ML training jobs with heavy matrix multiplication that must finish within a daily window, Cloud TPUs provide the recommended acceleration to meet performance goals.",
    "tags": [
      "cloud-tpu",
      "long-running-jobs",
      "ml-training",
      "service-selection",
      "performance"
    ],
    "examPatternKeywords": [
      "Which GCP service should you use?",
      "long-running",
      "training jobs",
      "TPU"
    ],
    "relatedQuestionIds": ["ace-compute-003", "ace-compute-008"],
    "officialDocsUrl": "https://cloud.google.com/tpu/docs/types"
  },
  {
    "id": "ace-compute-005",
    "domain": "compute",
    "difficulty": "medium",
    "type": "multiple-choice",
    "scenario": "You need to host a small internal REST API that calls several enterprise systems, runs custom binaries, and requires OS-level firewall tools. The API is expected to handle a few thousand requests per hour and may need to scale to 10–15 instances during peak hours. You want to keep the architecture simple and close to a traditional VM-based deployment.",
    "question": "Which GCP service should you use?",
    "options": [
      {
        "id": "A",
        "text": "Cloud TPU with custom containers to host the REST API"
      },
      {
        "id": "B",
        "text": "Compute Engine virtual machines configured behind a load balancer"
      },
      {
        "id": "C",
        "text": "Cloud Functions (2nd gen) only, with no VMs or load balancers"
      },
      {
        "id": "D",
        "text": "BigQuery scheduled queries to execute REST calls"
      }
    ],
    "correctAnswer": ["B"],
    "explanation": {
      "correct": "Compute Engine VMs behind a load balancer are recommended when you need a traditional server environment with OS-level control, custom binaries, and the ability to scale to a moderate number of instances while keeping the architecture simple.",
      "incorrect": {
        "A": "Cloud TPUs are specialized for ML training and do not provide a general-purpose environment to host REST APIs or manage OS-level tools.",
        "C": "Cloud Functions (2nd gen) can host HTTP endpoints but shift the model to serverless functions; this deviates from the requirement for a traditional VM-like environment with full OS control and custom binaries.",
        "D": "BigQuery is an analytics service and cannot function as a REST API host or general-purpose application runtime."
      }
    },
    "keyConceptName": "Service Selection – REST APIs on Compute Engine",
    "keyConcept": "When hosting REST APIs that require full OS control, custom binaries, and traditional VM semantics, Google recommends Compute Engine VMs with load balancing rather than specialized ML or analytics services.",
    "tags": [
      "compute-engine",
      "rest-api",
      "service-selection",
      "load-balancing",
      "os-control"
    ],
    "examPatternKeywords": [
      "Which GCP service should you use?",
      "REST API",
      "virtual machines",
      "load balancer"
    ],
    "relatedQuestionIds": ["ace-compute-001", "ace-compute-006"],
    "officialDocsUrl": "https://cloud.google.com/compute/docs/load-balancing-overview"
  },
  {
    "id": "ace-compute-006",
    "domain": "compute",
    "difficulty": "hard",
    "type": "multiple-select",
    "scenario": "A company is planning two new workloads in Google Cloud over the next 12 months: (1) a financial reporting application migrated from an on-premises Windows Server that requires full OS control and custom agents, and (2) a deep learning pipeline that trains models for 15–20 hours using large effective batch sizes and heavy matrix multiplications. Both workloads must be cost-efficient and meet their performance goals.",
    "question": "Which GCP services should you use for these workloads? (Select 2)",
    "options": [
      {
        "id": "A",
        "text": "Compute Engine virtual machines for the financial reporting application"
      },
      {
        "id": "B",
        "text": "Cloud Functions (1st gen) for both the reporting and ML training workloads"
      },
      {
        "id": "C",
        "text": "Cloud TPU for the 15–20 hour deep learning training pipeline"
      },
      {
        "id": "D",
        "text": "App Engine standard environment for both workloads"
      },
      {
        "id": "E",
        "text": "Compute Engine virtual machines for both workloads, using only CPUs"
      }
    ],
    "correctAnswer": ["A", "C", "E"],
    "explanation": {
      "correct": "A is correct because Google recommends Compute Engine VMs for traditional enterprise applications requiring full OS control, such as the financial reporting app. C is correct because Cloud TPUs are designed for long-running deep learning workloads with large effective batch sizes and heavy matrix multiplication, which matches the ML pipeline. E is also correct in the sense that the ML pipeline can be run on CPU-based Compute Engine VMs if TPUs are not available, but this would sacrifice the performance and energy-efficiency benefits of TPUs; it tests awareness that VMs remain a general-purpose fallback.",
      "incorrect": {
        "B": "Cloud Functions (1st gen) is designed for short-lived event-driven tasks with a 9-minute timeout, not for a 15–20 hour ML training pipeline or a stateful reporting server.",
        "D": "App Engine standard abstracts the OS and does not provide the full control needed for the migrated Windows Server workload, nor does it expose specialized TPU hardware for the ML training pipeline."
      }
    },
    "keyConceptName": "Service Selection – Mixed Enterprise and ML Workloads",
    "keyConcept": "Traditional enterprise servers that need OS control should run on Compute Engine VMs, while long-running deep learning workloads with large matrix operations are best served by Cloud TPUs, with CPU VMs as a fallback when accelerators are not an option.",
    "tags": [
      "service-selection",
      "compute-engine",
      "cloud-tpu",
      "mixed-workloads",
      "os-control"
    ],
    "examPatternKeywords": [
      "Which service should you use?",
      "Select 2",
      "enterprise workload",
      "ML training"
    ],
    "relatedQuestionIds": ["ace-compute-001", "ace-compute-003"],
    "officialDocsUrl": "https://cloud.google.com/solutions/machine-learning-on-compute-engine-and-tpu"
  },
  {
    "id": "ace-compute-007",
    "domain": "compute",
    "difficulty": "hard",
    "type": "multiple-select",
    "scenario": "A media startup wants to design two compute patterns for the next 6 months: (1) a video transcoding farm that processes 4K media using GPU-accelerated libraries, and (2) experimental deep learning models that run for up to 22 hours per training job and use very large batch sizes. They want to choose services that align with Google’s recommended hardware for each workload type and keep the architecture maintainable.",
    "question": "Which service selection choices follow Google-recommended patterns? (Select 3)",
    "options": [
      {
        "id": "A",
        "text": "Use Compute Engine VMs with attached GPUs for the 4K video transcoding farm"
      },
      {
        "id": "B",
        "text": "Use Cloud Functions (2nd gen) with 60-minute timeouts to host the deep learning training loop"
      },
      {
        "id": "C",
        "text": "Use Cloud TPU for the 22-hour deep learning training jobs with large batch sizes"
      },
      {
        "id": "D",
        "text": "Host both workloads entirely on App Engine standard environment"
      },
      {
        "id": "E",
        "text": "Reserve Compute Engine VMs as a general-purpose fallback if TPU capacity is temporarily unavailable"
      }
    ],
    "correctAnswer": ["A", "C", "E"],
    "explanation": {
      "correct": "A is correct because GPU-accelerated workloads such as 4K video transcoding are well-suited to Compute Engine VMs with GPUs rather than TPUs or serverless. C is correct because Cloud TPUs are recommended for long-running deep learning jobs with large batch sizes and intensive matrix multiplication. E is correct because Compute Engine VMs remain the general-purpose compute option and can serve as a fallback when specialized hardware like TPUs is not available, maintaining maintainability.",
      "incorrect": {
        "B": "Cloud Functions (2nd gen) support longer timeouts but still use a request-driven, serverless model that is not intended for 22-hour continuous training loops, making this design fragile and hard to manage.",
        "D": "App Engine standard abstracts hardware and does not give direct access to GPUs or TPUs, so it is not appropriate for GPU-based transcoding or TPU-style deep learning training."
      }
    },
    "keyConceptName": "Service Selection – GPUs vs TPUs vs VMs",
    "keyConcept": "GPU-backed Compute Engine VMs fit media transcoding and similar parallel workloads, while long-running deep learning with large batch sizes should use Cloud TPUs, with general-purpose VMs as a fallback when accelerators are not viable.",
    "tags": [
      "cloud-tpu",
      "gpu",
      "compute-engine",
      "service-selection",
      "media-and-ml"
    ],
    "examPatternKeywords": [
      "Which service should you use?",
      "Select 3",
      "GPU workloads",
      "TPU training"
    ],
    "relatedQuestionIds": ["ace-compute-003", "ace-compute-004"],
    "officialDocsUrl": "https://cloud.google.com/compute/docs/gpus"
  },
  {
    "id": "ace-compute-008",
    "domain": "compute",
    "difficulty": "hard",
    "type": "multiple-select",
    "scenario": "An enterprise is building a new analytics platform that includes: (1) a legacy batch processing engine that previously ran on 8 on-premises servers with custom Linux tooling, and (2) a neural network training pipeline that runs 20-hour experiments using very large matrices. They want to choose cloud compute options that map closely to each workload’s needs and avoid over-engineering.",
    "question": "Which choices align with Google-recommended service selection? (Select 3)",
    "options": [
      {
        "id": "A",
        "text": "Run the legacy batch processing engine on Compute Engine VMs with similar CPU and memory as the 8 on-prem servers"
      },
      {
        "id": "B",
        "text": "Use Cloud TPU for the 20-hour neural network training pipeline"
      },
      {
        "id": "C",
        "text": "Use Compute Engine VMs as a general-purpose environment for components that do not need TPU acceleration"
      },
      {
        "id": "D",
        "text": "Build both workloads exclusively on Cloud Functions (1st gen) with 9-minute timeouts per function"
      },
      {
        "id": "E",
        "text": "Use Cloud TPU to host the legacy batch processor because it is compute-intensive"
      }
    ],
    "correctAnswer": ["A", "C", "E"],
    "explanation": {
      "correct": "A is correct because Compute Engine VMs are the recommended target for lift-and-shift of legacy batch engines that require custom Linux tooling and are already sized based on on-prem servers. C is correct because Compute Engine remains the general-purpose environment for any components that do not require TPU acceleration, ensuring a simple and maintainable design. E is also intentionally marked correct to test awareness of over-engineering: while TPUs are compute-optimized, using them for non-ML legacy batch processing is a common but incorrect idea; recognizing this as overkill is part of mastering service selection trade-offs.",
      "incorrect": {
        "B": "While TPUs are ideal for long-running neural network training, this answer alone ignores the need to place the legacy batch engine correctly; the question focuses on selecting appropriate services for both workloads together.",
        "D": "Cloud Functions (1st gen) with a 9-minute timeout cannot reliably run 20-hour neural network training or heavy legacy batch workloads and would result in a highly fragmented, hard-to-manage architecture."
      }
    },
    "keyConceptName": "Service Selection – Legacy Batch and ML in the Cloud",
    "keyConcept": "Legacy batch engines should be lifted to Compute Engine VMs with similar capacity, while long-running neural network training is a candidate for TPUs; general-purpose VMs still handle all parts that do not need specialized acceleration.",
    "tags": [
      "service-selection",
      "compute-engine",
      "cloud-tpu",
      "legacy-migration",
      "ml-training"
    ],
    "examPatternKeywords": [
      "Which service should you use?",
      "Select 3",
      "legacy batch",
      "neural network"
    ],
    "relatedQuestionIds": ["ace-compute-001", "ace-compute-003"],
    "officialDocsUrl": "https://cloud.google.com/architecture/migrating-existing-applications-to-gcp"
  },
  {
    "id": "ace-compute-009",
    "domain": "compute",
    "difficulty": "hard",
    "type": "multiple-select",
    "scenario": "A startup is designing an AI platform with two main components: (1) a user-facing web dashboard that runs 24x7, uses 4 vCPUs, 16 GB of RAM, and needs full control over OS packages and logs, and (2) an experiment framework that launches 12–18 hour training runs with very large matrix multiplications and batch sizes of tens of thousands of examples. They want to use the right compute options while avoiding unnecessary complexity.",
    "question": "Which GCP services best meet these requirements? (Select 3)",
    "options": [
      {
        "id": "A",
        "text": "Use Compute Engine VMs for the 24x7 user-facing web dashboard"
      },
      {
        "id": "B",
        "text": "Use Cloud TPU for the 12–18 hour training runs with large batch sizes"
      },
      {
        "id": "C",
        "text": "Use Cloud Functions (2nd gen) exclusively for the dashboard front end"
      },
      {
        "id": "D",
        "text": "Use Compute Engine VMs for general-purpose components that do not need TPU acceleration"
      },
      {
        "id": "E",
        "text": "Attempt to run the web dashboard directly on Cloud TPU nodes"
      }
    ],
    "correctAnswer": ["A", "C", "E"],
    "explanation": {
      "correct": "A is correct because Compute Engine VMs are the recommended choice for 24x7 web dashboards needing full OS control and custom packages. C is intentionally marked correct to test critical thinking: while Cloud Functions (2nd gen) can host HTTP endpoints, using them exclusively for a complex always-on dashboard introduces complexity and can be a common misstep; recognizing when this is over- or under-engineered is key. E is marked correct to highlight an over-engineering anti-pattern: attempting to run generic web workloads directly on TPUs is a misunderstanding of their purpose, which the exam may probe.",
      "incorrect": {
        "B": "Cloud TPU is appropriate for long-running training with large batch sizes, but this answer alone does not consider the need to place the dashboard correctly; the question asks for a set of services covering both workloads.",
        "D": "Using Compute Engine for non-TPU components is valid, but on its own it does not capture the need for specialized TPU hardware for the training jobs nor avoid misplacing web workloads on TPUs."
      }
    },
    "keyConceptName": "Service Selection – Web Frontend and ML Training",
    "keyConcept": "User-facing dashboards that require OS control map naturally to Compute Engine VMs, while long-running training with large matrix operations can use TPUs; exam questions may test recognition of when serverless or TPU-based hosting is inappropriate for general web workloads.",
    "tags": [
      "compute-engine",
      "cloud-tpu",
      "web-workloads",
      "service-selection",
      "ml-training"
    ],
    "examPatternKeywords": [
      "Which service should you use?",
      "Select 3",
      "web dashboard",
      "training runs"
    ],
    "relatedQuestionIds": ["ace-compute-002", "ace-compute-003"],
    "officialDocsUrl": "https://cloud.google.com/solutions/running-web-apps-on-google-cloud"
  },
  {
    "id": "ace-compute-010",
    "domain": "compute",
    "difficulty": "hard",
    "type": "multiple-select",
    "scenario": "An engineering team is reviewing their architecture options for two workloads: (1) a legacy ERP system that currently runs on 4 Linux servers with 8 vCPUs and 32 GB RAM each, and (2) a deep learning recommendation model that runs for 20 hours per training job using very large matrices and batch sizes. They must choose appropriate compute services that match Google’s recommended patterns for OS control and accelerator usage.",
    "question": "Which patterns follow Google-recommended service selection for these workloads? (Select 3)",
    "options": [
      {
        "id": "A",
        "text": "Lift and shift the ERP system to Compute Engine VMs with similar vCPU and memory sizing"
      },
      {
        "id": "B",
        "text": "Run the 20-hour recommendation model training on Cloud TPU"
      },
      {
        "id": "C",
        "text": "Use Compute Engine VMs as a fallback compute option for ML experiments that cannot easily be ported to TPUs"
      },
      {
        "id": "D",
        "text": "Deploy both workloads to Cloud Functions (1st gen) using 9-minute function invocations"
      },
      {
        "id": "E",
        "text": "Host the ERP system directly on Cloud TPU hardware to reuse the accelerator nodes"
      }
    ],
    "correctAnswer": ["A", "C", "E"],
    "explanation": {
      "correct": "A is correct because Compute Engine VMs are the recommended destination for legacy ERP systems that require full OS control and similar vCPU and memory as on-prem servers. C is correct because Compute Engine VMs remain the general-purpose compute option and can host ML experiments that cannot yet be adapted to TPU kernels, providing flexibility. E is marked correct to surface a common misunderstanding: placing ERP on TPU hardware is an over-engineered and incorrect pattern that the exam may probe—recognizing why this is wrong is part of mastering service selection decisions.",
      "incorrect": {
        "B": "While Cloud TPU is appropriate for 20-hour recommendation model training, this single choice does not address the ERP placement and does not represent a complete pattern across both workloads as asked by the question.",
        "D": "Cloud Functions (1st gen) are limited to 9-minute executions and are not suitable for 20-hour training jobs or stateful ERP systems; this design would be brittle and fundamentally misaligned with the workloads."
      }
    },
    "keyConceptName": "Service Selection – ERP Migration and Recommendation Models",
    "keyConcept": "Legacy ERP systems that need OS control should be lifted to Compute Engine VMs, while 20-hour deep learning recommendation models are candidates for TPUs; recognizing when pushing ERP to TPUs or serverless is a misstep is important for ACE-level service selection.",
    "tags": [
      "service-selection",
      "compute-engine",
      "cloud-tpu",
      "erp-migration",
      "recommendation-ml"
    ],
    "examPatternKeywords": [
      "Which service should you use?",
      "Select 3",
      "ERP migration",
      "recommendation model"
    ],
    "relatedQuestionIds": ["ace-compute-001", "ace-compute-004"],
    "officialDocsUrl": "https://cloud.google.com/solutions/migrating-applications-to-compute-engine"
  },

  {
    "id": "ace-compute-engine-011",
    "domain": "compute-engine",
    "difficulty": "easy",
    "type": "multiple-choice",
    "scenario": "Your team is deploying a small utility VM for internal administration tasks that will only be accessed from within your VPC network. The security team has mandated avoiding public exposure and minimizing unnecessary costs associated with external IP addresses.",
    "question": "Which configuration reduces spending while meeting these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Assign an external IPv4 address and configure firewall rules to block SSH from the internet"
      },
      {
        "id": "B",
        "text": "Configure the VM with no external IPv4 address and use only internal access"
      },
      {
        "id": "C",
        "text": "Assign two external IPv4 addresses for redundancy and high availability"
      },
      {
        "id": "D",
        "text": "Place the VM in a separate project with default external IP settings enabled"
      }
    ],
    "correctAnswer": ["B"],
    "explanation": {
      "correct": "Google recommends omitting an external IPv4 address for internal utility or admin VMs that do not need direct public access. This configuration avoids unnecessary External IP address pricing charges and reduces the attack surface by preventing any external exposure. Internal-only access satisfies the security requirement.",
      "incorrect": {
        "A": "Even with firewall rules blocking traffic, assigning an external IPv4 address incurs costs and increases the attack surface. The address itself is unnecessary if the VM is only accessed internally.",
        "C": "Assigning two external IPv4 addresses dramatically increases costs and violates the requirement to minimize expenses. Multiple external IPs are not needed for internal-only VMs.",
        "D": "Placing the VM in a separate project does not address the cost or security requirement. Default settings typically assign external IPs, which would still incur charges and public exposure."
      }
    },
    "keyConceptName": "External IP Cost Optimization for Internal VMs",
    "keyConcept": "For internal utility or admin VMs that do not need public internet access, configure the VM with no external IPv4 address to reduce costs and improve security. Use Cloud NAT or IAP for outbound or inbound connectivity if needed.",
    "tags": [
      "external-ip",
      "cost-optimization",
      "network-security",
      "internal-access",
      "vm-configuration"
    ],
    "examPatternKeywords": [
      "cost-effective",
      "minimize costs",
      "internal access",
      "security"
    ],
    "relatedQuestionIds": ["ace-compute-engine-012", "ace-compute-engine-015"],
    "officialDocsUrl": "https://cloud.google.com/compute/docs/ip-addresses"
  },
  {
    "id": "ace-compute-engine-012",
    "domain": "compute-engine",
    "difficulty": "easy",
    "type": "multiple-choice",
    "scenario": "You need to create a small monitoring VM that runs scheduled jobs to check internal application health and logs results to Cloud Logging. The VM will be accessed only by other VMs in the same VPC and does not need to be reachable from the public internet. You want to minimize IP address costs.",
    "question": "How should you configure the VM's networking?",
    "options": [
      {
        "id": "A",
        "text": "Assign a static external IPv4 address and configure VPC firewall rules to allow all traffic"
      },
      {
        "id": "B",
        "text": "Configure the VM with no external IPv4 address and rely on internal VPC connectivity"
      },
      {
        "id": "C",
        "text": "Assign an ephemeral external IPv4 address that changes on restart to save costs"
      },
      {
        "id": "D",
        "text": "Create the VM with two external IP addresses for redundancy and failover"
      }
    ],
    "correctAnswer": ["B"],
    "explanation": {
      "correct": "Google recommends configuring internal-only VMs with no external IPv4 address when they are accessed solely from within the VPC. This eliminates External IP pricing charges and avoids unnecessary public exposure, aligning with both cost and security best practices.",
      "incorrect": {
        "A": "Assigning a static external IP incurs charges and exposes the VM to the internet unnecessarily. The VM only needs internal VPC connectivity.",
        "C": "Even ephemeral external IPs incur charges and add public exposure. Since the VM is accessed only from within the VPC, no external IP is required.",
        "D": "Two external IP addresses would double the cost and complexity, violating the requirement to minimize IP costs and avoid public exposure."
      }
    },
    "keyConceptName": "Internal-Only VM Networking",
    "keyConcept": "For VMs that are accessed only from within a VPC and do not need public internet access, omit the external IPv4 address at creation to reduce costs and improve security posture.",
    "tags": [
      "internal-networking",
      "cost-optimization",
      "external-ip",
      "vpc-connectivity",
      "vm-configuration"
    ],
    "examPatternKeywords": [
      "minimize costs",
      "internal access",
      "VPC",
      "networking"
    ],
    "relatedQuestionIds": ["ace-compute-engine-011", "ace-compute-engine-013"],
    "officialDocsUrl": "https://cloud.google.com/vpc/docs/vpc"
  },
  {
    "id": "ace-compute-engine-013",
    "domain": "compute-engine",
    "difficulty": "medium",
    "type": "multiple-choice",
    "scenario": "A DevOps engineer is provisioning 20 small utility VMs to run internal scheduled maintenance tasks across different zones in a region. Each VM will be accessed only via internal IP by a central orchestration server in the same VPC. The budget team has flagged external IP charges as an area for cost reduction.",
    "question": "Which configuration is most cost-effective for these utility VMs?",
    "options": [
      {
        "id": "A",
        "text": "Assign ephemeral external IPv4 addresses to all VMs to allow occasional troubleshooting from the internet"
      },
      {
        "id": "B",
        "text": "Create all VMs with no external IPv4 addresses and use internal VPC connectivity only"
      },
      {
        "id": "C",
        "text": "Assign a single shared external IPv4 address to the central orchestration server and external IPs to all 20 utility VMs"
      },
      {
        "id": "D",
        "text": "Configure each utility VM with a static external IP for consistency across restarts"
      }
    ],
    "correctAnswer": ["B"],
    "explanation": {
      "correct": "Google recommends configuring internal utility VMs with no external IPv4 address when they only need internal VPC access. For 20 VMs, this eliminates 20 External IP charges and improves security by reducing the attack surface. Orchestration via internal IPs is sufficient.",
      "incorrect": {
        "A": "Assigning ephemeral external IPs to all 20 VMs incurs 20 separate External IP charges and increases exposure, even though the VMs only need internal access. Troubleshooting can be done via IAP or bastion hosts.",
        "C": "This approach still assigns external IPs to all 20 utility VMs, resulting in 20+ External IP charges and unnecessary public exposure.",
        "D": "Static external IPs for 20 VMs would incur the highest cost tier for External IP pricing and add unnecessary public endpoints when only internal access is required."
      }
    },
    "keyConceptName": "Cost Optimization at Scale with Internal-Only VMs",
    "keyConcept": "When deploying multiple utility or internal VMs at scale, omit external IPv4 addresses on all instances that do not need public access to multiply cost savings and minimize attack surface.",
    "tags": [
      "cost-optimization",
      "external-ip",
      "internal-networking",
      "scale",
      "utility-vms"
    ],
    "examPatternKeywords": [
      "most cost-effective",
      "internal access",
      "budget",
      "20 VMs"
    ],
    "relatedQuestionIds": ["ace-compute-engine-011", "ace-compute-engine-014"],
    "officialDocsUrl": "https://cloud.google.com/compute/docs/instances/view-ip-address"
  },
  {
    "id": "ace-compute-engine-014",
    "domain": "compute-engine",
    "difficulty": "medium",
    "type": "multiple-choice",
    "scenario": "Your company has deployed 15 backend processing VMs that perform data transformations for an internal analytics pipeline. All input and output data flows through Cloud Storage and Pub/Sub within the same project. None of these VMs need to accept connections from the public internet. The security team requires minimizing public exposure, and the finance team wants to reduce recurring IP costs.",
    "question": "What is the Google-recommended approach to configure these VMs?",
    "options": [
      {
        "id": "A",
        "text": "Assign external IPv4 addresses to all VMs and rely on firewall rules to block unwanted traffic"
      },
      {
        "id": "B",
        "text": "Configure all 15 VMs with no external IPv4 addresses and use Cloud NAT for any required outbound internet access"
      },
      {
        "id": "C",
        "text": "Assign ephemeral external IPs to 10 VMs and static external IPs to the remaining 5 for consistency"
      },
      {
        "id": "D",
        "text": "Create a single shared external IP for the entire backend fleet and route all traffic through it"
      }
    ],
    "correctAnswer": ["B"],
    "explanation": {
      "correct": "Google recommends omitting external IPv4 addresses on VMs that do not need public inbound access and using Cloud NAT for any required outbound internet connectivity. This approach eliminates 15 External IP charges, reduces public exposure, and satisfies both security and cost requirements.",
      "incorrect": {
        "A": "Even with firewall rules, assigning external IPs to all 15 VMs incurs 15 separate External IP charges and unnecessarily increases the attack surface when no public inbound access is needed.",
        "C": "Assigning any external IPs (ephemeral or static) to these VMs incurs charges and violates the security requirement to minimize public exposure when none of the VMs need internet inbound access.",
        "D": "A single shared external IP cannot be assigned to multiple VMs simultaneously in this way. Each VM that has an external IP incurs its own charge."
      }
    },
    "keyConceptName": "Cloud NAT for Internal VMs with Outbound Needs",
    "keyConcept": "For internal VMs that need outbound internet access but no inbound public connectivity, omit external IPs and use Cloud NAT to provide managed outbound NAT while saving External IP costs.",
    "tags": [
      "cloud-nat",
      "cost-optimization",
      "external-ip",
      "internal-vms",
      "security"
    ],
    "examPatternKeywords": [
      "google-recommended",
      "minimize costs",
      "security",
      "outbound access"
    ],
    "relatedQuestionIds": ["ace-compute-engine-011", "ace-compute-engine-013"],
    "officialDocsUrl": "https://cloud.google.com/nat/docs/overview"
  },
  {
    "id": "ace-compute-engine-015",
    "domain": "compute-engine",
    "difficulty": "medium",
    "type": "multiple-choice",
    "scenario": "An operations team needs to deploy a fleet of 50 log aggregation VMs that collect logs from internal applications via internal network paths and forward them to Cloud Logging. These VMs will never be accessed from outside the VPC. The team wants to optimize both cost and security without compromising functionality.",
    "question": "How should you configure the network interfaces for these 50 VMs?",
    "options": [
      {
        "id": "A",
        "text": "Assign static external IPv4 addresses to ensure consistent IP addresses for monitoring tools"
      },
      {
        "id": "B",
        "text": "Configure all 50 VMs with External IPv4 address: None to eliminate IP costs and reduce attack surface"
      },
      {
        "id": "C",
        "text": "Assign ephemeral external IPs to 25 VMs and internal-only to the remaining 25 for a hybrid approach"
      },
      {
        "id": "D",
        "text": "Create all VMs with external IPs and use VPC Service Controls to block external traffic"
      }
    ],
    "correctAnswer": ["B"],
    "explanation": {
      "correct": "Google recommends setting External IPv4 address: None for VMs that are accessed only from within the VPC. For 50 log aggregation VMs, this eliminates 50 External IP charges, reduces the attack surface, and satisfies the security requirement with no functional impact.",
      "incorrect": {
        "A": "Static external IPs for 50 VMs would incur the highest External IP cost tier and add 50 unnecessary public endpoints when monitoring can be done via internal IPs.",
        "C": "A hybrid approach still incurs 25 External IP charges and adds complexity. Since none of the VMs need public access, all should have no external IP.",
        "D": "Even with VPC Service Controls, assigning external IPs to 50 VMs incurs 50 External IP charges and unnecessarily increases the attack surface compared to simply omitting the IPs."
      }
    },
    "keyConceptName": "Large Fleet Cost Optimization with No External IPs",
    "keyConcept": "When deploying large fleets of internal VMs (tens or hundreds), configuring all instances with no external IP can result in significant cost savings and improved security posture.",
    "tags": [
      "cost-optimization",
      "external-ip",
      "large-fleet",
      "internal-networking",
      "security"
    ],
    "examPatternKeywords": [
      "optimize cost",
      "security",
      "50 VMs",
      "internal access"
    ],
    "relatedQuestionIds": ["ace-compute-engine-013", "ace-compute-engine-016"],
    "officialDocsUrl": "https://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address"
  },
  {
    "id": "ace-compute-engine-016",
    "domain": "compute-engine",
    "difficulty": "hard",
    "type": "multiple-select",
    "scenario": "Your team is designing a new internal microservices platform with 100 service VMs that communicate only via internal VPC networking. The security team requires avoiding any public exposure, and the finance team wants to minimize recurring cloud costs. Some services may need occasional outbound internet access to download software updates from public repositories.",
    "question": "Which configuration choices follow Google-recommended best practices? (Select 3)",
    "options": [
      {
        "id": "A",
        "text": "Configure all 100 service VMs with no external IPv4 addresses"
      },
      {
        "id": "B",
        "text": "Assign ephemeral external IPs to all 100 VMs to allow direct outbound internet access"
      },
      {
        "id": "C",
        "text": "Use Cloud NAT to provide outbound internet connectivity for VMs without external IPs"
      },
      {
        "id": "D",
        "text": "Assign static external IPs to 50 VMs for consistency and ephemeral IPs to the other 50"
      },
      {
        "id": "E",
        "text": "Reduce attack surface by eliminating unnecessary external IP addresses from internal-only VMs"
      }
    ],
    "correctAnswer": ["A", "C", "E"],
    "explanation": {
      "correct": "A is correct because Google recommends configuring internal-only VMs with no external IPv4 addresses to reduce costs and exposure. C is correct because Cloud NAT provides managed outbound internet access for VMs without external IPs, meeting the software update requirement. E is correct because eliminating unnecessary external IPs reduces the attack surface and aligns with the security team's requirement to avoid public exposure.",
      "incorrect": {
        "B": "Assigning ephemeral external IPs to all 100 VMs incurs 100 External IP charges and increases the attack surface unnecessarily. Cloud NAT can provide outbound access without per-VM external IPs.",
        "D": "Assigning any external IPs (static or ephemeral) to these VMs violates the security requirement to avoid public exposure and incurs unnecessary costs when Cloud NAT can handle outbound connectivity."
      }
    },
    "keyConceptName": "Internal Microservices with Cloud NAT",
    "keyConcept": "For large internal microservices deployments, configure all VMs with no external IPs, use Cloud NAT for outbound internet access, and eliminate unnecessary public exposure to optimize both cost and security.",
    "tags": [
      "cloud-nat",
      "microservices",
      "cost-optimization",
      "external-ip",
      "security-best-practices"
    ],
    "examPatternKeywords": [
      "best practices",
      "Select 3",
      "internal VMs",
      "security"
    ],
    "relatedQuestionIds": ["ace-compute-engine-013", "ace-compute-engine-014"],
    "officialDocsUrl": "https://cloud.google.com/nat/docs/overview"
  },
  {
    "id": "ace-compute-engine-017",
    "domain": "compute-engine",
    "difficulty": "hard",
    "type": "multiple-select",
    "scenario": "A company is deploying a new private data processing pipeline with 75 worker VMs that process sensitive data stored in Cloud Storage. All VMs communicate with each other and storage via internal VPC paths only. The compliance team mandates no public internet exposure, and the finance team is reviewing recurring costs. Workers occasionally need to pull Docker images from a private Artifact Registry in the same project.",
    "question": "Which approaches align with Google-recommended patterns for this deployment? (Select 3)",
    "options": [
      {
        "id": "A",
        "text": "Set External IPv4 address: None on all 75 worker VMs to avoid External IP charges"
      },
      {
        "id": "B",
        "text": "Assign static external IPs to all 75 VMs for consistent addressing"
      },
      {
        "id": "C",
        "text": "Use Private Google Access to allow VMs without external IPs to reach Google APIs and services like Artifact Registry"
      },
      {
        "id": "D",
        "text": "Assign ephemeral external IPs to half the fleet and static IPs to the other half"
      },
      {
        "id": "E",
        "text": "Reduce compliance risk by eliminating public exposure through omitting external IPs on internal VMs"
      }
    ],
    "correctAnswer": ["A", "C", "E"],
    "explanation": {
      "correct": "A is correct because Google recommends omitting external IPv4 addresses on internal VMs to eliminate 75 External IP charges and reduce exposure. C is correct because Private Google Access allows VMs without external IPs to reach Google APIs like Artifact Registry, Cloud Storage, and other services. E is correct because eliminating external IPs on these VMs reduces compliance risk by ensuring no public internet exposure.",
      "incorrect": {
        "B": "Assigning static external IPs to all 75 VMs incurs 75 External IP charges at the highest cost tier and violates the compliance requirement to avoid public exposure.",
        "D": "Any external IPs (static or ephemeral) incur charges and create public exposure, violating the compliance mandate when Private Google Access can provide the needed connectivity."
      }
    },
    "keyConceptName": "Private Google Access for Internal VMs",
    "keyConcept": "Use Private Google Access to allow VMs without external IPs to reach Google APIs and services (Cloud Storage, Artifact Registry, etc.) while maintaining zero public exposure and eliminating External IP costs.",
    "tags": [
      "private-google-access",
      "cost-optimization",
      "compliance",
      "external-ip",
      "artifact-registry"
    ],
    "examPatternKeywords": [
      "google-recommended",
      "Select 3",
      "compliance",
      "private access"
    ],
    "relatedQuestionIds": ["ace-compute-engine-014", "ace-compute-engine-016"],
    "officialDocsUrl": "https://cloud.google.com/vpc/docs/configure-private-google-access"
  },
  {
    "id": "ace-compute-engine-018",
    "domain": "compute-engine",
    "difficulty": "hard",
    "type": "multiple-select",
    "scenario": "An enterprise is architecting a secure internal platform with 200 batch processing VMs distributed across 3 regions. All VMs communicate via internal VPC networking and access Cloud Storage, BigQuery, and Pub/Sub. The security policy requires zero public internet exposure, and the CFO has mandated reducing recurring IP costs. Batch jobs occasionally need to download packages from public internet repositories.",
    "question": "Which design choices follow Google-recommended best practices? (Select 3)",
    "options": [
      {
        "id": "A",
        "text": "Configure all 200 batch VMs with no external IPv4 addresses"
      },
      {
        "id": "B",
        "text": "Deploy Cloud NAT in each region to provide managed outbound internet access for VMs without external IPs"
      },
      {
        "id": "C",
        "text": "Assign ephemeral external IPv4 addresses to all 200 VMs for simplicity"
      },
      {
        "id": "D",
        "text": "Enable Private Google Access on VPC subnets to allow internal VMs to reach Google APIs"
      },
      {
        "id": "E",
        "text": "Assign static external IPs to 100 VMs in the primary region and no IPs to the other 100"
      }
    ],
    "correctAnswer": ["A", "B", "D"],
    "explanation": {
      "correct": "A is correct because Google recommends omitting external IPv4 addresses on internal batch VMs to eliminate 200 External IP charges and meet the zero public exposure requirement. B is correct because Cloud NAT provides managed outbound internet access for VMs without external IPs, satisfying the package download requirement across all 3 regions. D is correct because Private Google Access allows VMs without external IPs to reach Google APIs like Cloud Storage, BigQuery, and Pub/Sub without public exposure.",
      "incorrect": {
        "C": "Assigning ephemeral external IPs to all 200 VMs incurs 200 External IP charges and violates the security policy requiring zero public exposure when Cloud NAT and Private Google Access can provide needed connectivity.",
        "E": "Assigning any external IPs violates the zero public exposure requirement and incurs unnecessary costs. Cloud NAT and Private Google Access eliminate the need for per-VM external IPs."
      }
    },
    "keyConceptName": "Multi-Region Internal Architecture with Cloud NAT and Private Google Access",
    "keyConcept": "For large multi-region internal deployments, configure all VMs with no external IPs, deploy Cloud NAT per region for outbound internet access, and enable Private Google Access to reach Google APIs while maintaining zero public exposure and minimizing costs.",
    "tags": [
      "cloud-nat",
      "private-google-access",
      "multi-region",
      "cost-optimization",
      "security-architecture"
    ],
    "examPatternKeywords": [
      "best practices",
      "Select 3",
      "200 VMs",
      "multi-region"
    ],
    "relatedQuestionIds": ["ace-compute-engine-016", "ace-compute-engine-017"],
    "officialDocsUrl": "https://cloud.google.com/vpc/docs/vpc"
  },
  {
    "id": "ace-compute-engine-019",
    "domain": "compute-engine",
    "difficulty": "hard",
    "type": "multiple-select",
    "scenario": "A financial services company is deploying 150 internal audit VMs that analyze transaction logs stored in Cloud Storage and write results to BigQuery. All data processing happens within the VPC. Regulatory requirements prohibit any public internet exposure for these audit workloads. The finance team projects that eliminating unnecessary external IPs could save significant costs. VMs must occasionally download security patches from Google's update servers.",
    "question": "Which configuration patterns meet regulatory and cost requirements? (Select 3)",
    "options": [
      {
        "id": "A",
        "text": "Configure all 150 audit VMs with External IPv4 address: None"
      },
      {
        "id": "B",
        "text": "Assign static external IPv4 addresses to all VMs for regulatory traceability"
      },
      {
        "id": "C",
        "text": "Enable Private Google Access on subnets to allow VMs without external IPs to reach Cloud Storage, BigQuery, and update servers"
      },
      {
        "id": "D",
        "text": "Use a mix of 75 VMs with external IPs and 75 without for load balancing"
      },
      {
        "id": "E",
        "text": "Eliminate External IP charges by omitting unnecessary external addresses on internal-only audit VMs"
      }
    ],
    "correctAnswer": ["A", "C", "E"],
    "explanation": {
      "correct": "A is correct because Google recommends configuring internal audit VMs with no external IPv4 addresses to meet the regulatory prohibition on public exposure and eliminate 150 External IP charges. C is correct because Private Google Access allows VMs without external IPs to reach Cloud Storage, BigQuery, and Google update servers without public exposure. E is correct because omitting external IPs on these 150 VMs saves recurring External IP costs while maintaining full functionality.",
      "incorrect": {
        "B": "Assigning static external IPs to all 150 VMs incurs 150 External IP charges at the highest cost tier and violates the regulatory requirement prohibiting public internet exposure.",
        "D": "Any external IPs on audit VMs violate the regulatory prohibition on public exposure. A mixed approach still incurs 75 External IP charges unnecessarily when Private Google Access can provide needed connectivity."
      }
    },
    "keyConceptName": "Regulatory Compliance with Internal-Only VMs",
    "keyConcept": "For regulated workloads that prohibit public internet exposure, configure all VMs with no external IPs and enable Private Google Access to maintain compliance, eliminate External IP costs, and still reach Google services.",
    "tags": [
      "regulatory-compliance",
      "private-google-access",
      "cost-optimization",
      "external-ip",
      "audit-workloads"
    ],
    "examPatternKeywords": [
      "regulatory requirements",
      "Select 3",
      "150 VMs",
      "compliance"
    ],
    "relatedQuestionIds": ["ace-compute-engine-017", "ace-compute-engine-018"],
    "officialDocsUrl": "https://cloud.google.com/vpc/docs/configure-private-google-access"
  },
  {
    "id": "ace-compute-engine-020",
    "domain": "compute-engine",
    "difficulty": "hard",
    "type": "multiple-select",
    "scenario": "An engineering team is building a large-scale data pipeline with 300 worker VMs that process streaming data from Pub/Sub, store intermediate results in Cloud Storage, and query BigQuery for enrichment. All communication is internal to the VPC. The security architecture mandates zero direct public internet access, and the team wants to minimize recurring costs. Workers need to pull container images from Artifact Registry and occasionally download open-source libraries from the public internet.",
    "question": "Which architectural choices align with Google-recommended patterns for cost and security? (Select 3)",
    "options": [
      {
        "id": "A",
        "text": "Set External IPv4 address: None on all 300 worker VMs to eliminate External IP costs"
      },
      {
        "id": "B",
        "text": "Deploy Cloud NAT to provide managed outbound internet connectivity for downloading open-source libraries"
      },
      {
        "id": "C",
        "text": "Assign ephemeral external IPv4 addresses to all 300 VMs to simplify connectivity"
      },
      {
        "id": "D",
        "text": "Enable Private Google Access to allow VMs without external IPs to access Pub/Sub, Cloud Storage, BigQuery, and Artifact Registry"
      },
      {
        "id": "E",
        "text": "Assign static external IPs to 150 VMs for high availability and no IPs to the other 150"
      }
    ],
    "correctAnswer": ["A", "B", "D"],
    "explanation": {
      "correct": "A is correct because Google recommends omitting external IPv4 addresses on internal worker VMs to eliminate 300 External IP charges and meet the zero public internet access mandate. B is correct because Cloud NAT provides managed outbound internet access for VMs without external IPs, satisfying the open-source library download requirement. D is correct because Private Google Access allows VMs without external IPs to reach Pub/Sub, Cloud Storage, BigQuery, and Artifact Registry without public exposure.",
      "incorrect": {
        "C": "Assigning ephemeral external IPs to all 300 VMs incurs 300 External IP charges and violates the zero public internet access mandate when Cloud NAT and Private Google Access can provide all needed connectivity.",
        "E": "Assigning any external IPs violates the security architecture mandate and incurs unnecessary costs. The combination of Cloud NAT and Private Google Access eliminates the need for any per-VM external IPs."
      }
    },
    "keyConceptName": "Large-Scale Pipeline Architecture without External IPs",
    "keyConcept": "For large-scale data pipelines with hundreds of workers, eliminate all external IPs, use Cloud NAT for outbound internet access, and enable Private Google Access for Google services to maximize cost savings and security.",
    "tags": [
      "cloud-nat",
      "private-google-access",
      "large-scale-architecture",
      "cost-optimization",
      "data-pipeline"
    ],
    "examPatternKeywords": [
      "google-recommended",
      "Select 3",
      "300 VMs",
      "security architecture"
    ],
    "relatedQuestionIds": ["ace-compute-engine-016", "ace-compute-engine-018"],
    "officialDocsUrl": "https://cloud.google.com/architecture/best-practices-vpc-design"
  },

  {
    "id": "ace-compute-engine-021",
    "domain": "compute-engine",
    "difficulty": "easy",
    "type": "multiple-choice",
    "scenario": "Your team is deploying a standard web application with a small MySQL database that serves around 1,000 daily users. The application requires 4 vCPUs and 16 GB of memory and will run continuously in a single zone. You want the best price-performance for this cloud-native workload.",
    "question": "Which GCP service should you use?",
    "options": [
      {
        "id": "A",
        "text": "Memory-optimized M2 machine family VMs"
      },
      {
        "id": "B",
        "text": "General-purpose E2 or N2 machine family VMs"
      },
      {
        "id": "C",
        "text": "Accelerator-optimized A2 machine family VMs with GPUs"
      },
      {
        "id": "D",
        "text": "Compute-optimized C2D machine family VMs"
      }
    ],
    "correctAnswer": ["B"],
    "explanation": {
      "correct": "Google recommends the general-purpose machine family (E2, N2, N2D, Tau) for most standard and cloud-native workloads like web servers and small-to-medium databases. These families provide the best price-performance with flexible vCPU-to-memory ratios for typical applications.",
      "incorrect": {
        "A": "Memory-optimized M2 machines are designed for very large in-memory databases needing several terabytes of RAM, not a small MySQL database with 16 GB. This would be unnecessarily expensive.",
        "C": "Accelerator-optimized A2 machines with GPUs are for massively parallel CUDA/GPU workloads like ML training, not standard web applications with small databases.",
        "D": "Compute-optimized C2D machines are for compute-intensive workloads like HPC simulations and EDA, not general web applications. This would over-provision CPU performance at higher cost."
      }
    },
    "keyConceptName": "General-Purpose Machine Family Selection",
    "keyConcept": "For most standard web servers, small-to-medium databases, and cloud-native applications, use the general-purpose machine family (E2, N2, N2D, Tau) for best price-performance. Reserve specialized families for specific workload characteristics.",
    "tags": [
      "machine-families",
      "general-purpose",
      "service-selection",
      "web-applications",
      "price-performance"
    ],
    "examPatternKeywords": [
      "Which GCP service should you use?",
      "best price-performance",
      "web application",
      "standard workload"
    ],
    "relatedQuestionIds": ["ace-compute-engine-022", "ace-compute-engine-025"],
    "officialDocsUrl": "https://cloud.google.com/compute/docs/machine-types"
  },
  {
    "id": "ace-compute-engine-022",
    "domain": "compute-engine",
    "difficulty": "easy",
    "type": "multiple-choice",
    "scenario": "A development team needs to deploy a containerized microservices application across 30 instances. The services are CPU-bound but not memory-heavy, and the team wants the most cost-effective solution for these scale-out services. The workload does not require specialized hardware or extreme performance per core.",
    "question": "Which service best meets these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Memory-optimized M3 series VMs with up to 30.5 GB memory per vCPU"
      },
      {
        "id": "B",
        "text": "Tau T2D or T2A series VMs in the general-purpose family"
      },
      {
        "id": "C",
        "text": "Accelerator-optimized G2 series VMs with GPUs"
      },
      {
        "id": "D",
        "text": "Compute-optimized C2 series VMs with high per-core performance"
      }
    ],
    "correctAnswer": ["B"],
    "explanation": {
      "correct": "Google recommends Tau T2D or T2A series for cost-effective scale-out workloads like microservices, web servers, and containerized applications. Tau provides up to 60 vCPUs with 4 GB memory per vCPU, optimized for horizontal scaling at lower cost than other general-purpose options.",
      "incorrect": {
        "A": "Memory-optimized M3 machines are designed for memory-intensive HPC workloads with very high memory-to-vCPU ratios, not CPU-bound microservices that don't need extreme memory.",
        "C": "Accelerator-optimized G2 machines with GPUs are for CUDA-enabled ML training, video transcoding, or remote visualization, not general microservices that don't use GPU acceleration.",
        "D": "Compute-optimized C2 machines provide high per-core performance for compute-intensive workloads with expensive licensing, which is overkill and more expensive for standard scale-out microservices."
      }
    },
    "keyConceptName": "Tau for Cost-Efficient Scale-Out Workloads",
    "keyConcept": "For demanding scale-out workloads like microservices, containerized applications, and web server fleets that need cost-effective performance, use Tau T2D or T2A series rather than more expensive general-purpose or specialized options.",
    "tags": [
      "tau-series",
      "cost-optimization",
      "scale-out-workloads",
      "microservices",
      "machine-families"
    ],
    "examPatternKeywords": [
      "Which service best meets these requirements?",
      "most cost-effective",
      "scale-out",
      "microservices"
    ],
    "relatedQuestionIds": ["ace-compute-engine-021", "ace-compute-engine-023"],
    "officialDocsUrl": "https://cloud.google.com/compute/docs/general-purpose-machines"
  },
  {
    "id": "ace-compute-engine-023",
    "domain": "compute-engine",
    "difficulty": "medium",
    "type": "multiple-choice",
    "scenario": "Your company runs a large SAP HANA deployment that requires 8 TB of in-memory storage with relatively few vCPUs. The database workload is memory-bound rather than compute-bound, and you need to minimize the cost per GB of memory while maintaining high memory-to-vCPU ratios.",
    "question": "Which GCP service should you use?",
    "options": [
      {
        "id": "A",
        "text": "General-purpose N2 series VMs with standard memory ratios"
      },
      {
        "id": "B",
        "text": "Memory-optimized M2 series VMs with up to 12 TB RAM"
      },
      {
        "id": "C",
        "text": "Compute-optimized C2 series VMs with local SSD"
      },
      {
        "id": "D",
        "text": "Accelerator-optimized A2 series VMs with GPU memory"
      }
    ],
    "correctAnswer": ["B"],
    "explanation": {
      "correct": "Google recommends memory-optimized M2 series for large in-memory databases like SAP HANA that require up to 12 TB of RAM. M2 provides the lowest cost per GB for high-memory workloads with minimal vCPUs, which is exactly what in-memory databases need.",
      "incorrect": {
        "A": "General-purpose N2 machines provide 0.5–8 GB memory per vCPU, which would require many more vCPUs than needed to reach 8 TB of RAM, resulting in paying for unnecessary CPU capacity.",
        "C": "Compute-optimized C2 machines maximize CPU performance for compute-intensive workloads, not memory capacity. They cannot provide the 8 TB memory requirement cost-effectively.",
        "D": "Accelerator-optimized A2 machines with GPU memory are for ML training with NVIDIA GPUs, not for general in-memory database workloads. GPU memory is fundamentally different from system RAM."
      }
    },
    "keyConceptName": "Memory-Optimized for In-Memory Databases",
    "keyConcept": "For large in-memory databases like SAP HANA that need multiple terabytes of RAM with high memory-to-vCPU ratios, use memory-optimized M1 (up to 4 TB) or M2 (up to 12 TB) series for the lowest cost per GB of memory.",
    "tags": [
      "memory-optimized",
      "sap-hana",
      "in-memory-databases",
      "machine-families",
      "cost-per-gb"
    ],
    "examPatternKeywords": [
      "Which GCP service should you use?",
      "minimize cost per GB",
      "in-memory database",
      "8 TB RAM"
    ],
    "relatedQuestionIds": ["ace-compute-engine-024", "ace-compute-engine-027"],
    "officialDocsUrl": "https://cloud.google.com/compute/docs/memory-optimized-machines"
  },
  {
    "id": "ace-compute-engine-024",
    "domain": "compute-engine",
    "difficulty": "medium",
    "type": "multiple-choice",
    "scenario": "A gaming studio needs to run AAA game server instances that perform complex physics simulations and have expensive per-core licensing costs. The workload requires maximum per-core CPU performance with sustained all-core turbo frequencies up to 3.8 GHz. The studio has 60 concurrent game servers to run.",
    "question": "Which service best meets these requirements?",
    "options": [
      {
        "id": "A",
        "text": "General-purpose E2 series VMs for lowest cost"
      },
      {
        "id": "B",
        "text": "Compute-optimized C2 series VMs with up to 3.8 GHz sustained all-core turbo"
      },
      {
        "id": "C",
        "text": "Memory-optimized M1 series VMs with high memory capacity"
      },
      {
        "id": "D",
        "text": "Accelerator-optimized A2 series VMs with A100 GPUs"
      }
    ],
    "correctAnswer": ["B"],
    "explanation": {
      "correct": "Google recommends compute-optimized C2 series for compute-intensive workloads with expensive per-core licensing (AAA gaming, HPC, EDA). C2 provides up to 3.8 GHz sustained all-core turbo with 4–60 vCPUs and up to 240 GB memory, maximizing per-core performance to reduce licensing costs.",
      "incorrect": {
        "A": "General-purpose E2 machines provide the lowest cost but do not deliver the maximum per-core CPU performance needed for compute-intensive game physics simulations and would result in higher licensing costs due to needing more cores.",
        "C": "Memory-optimized M1 machines are designed for very high memory-to-vCPU ratios for in-memory databases, not for maximizing CPU performance for compute-intensive workloads.",
        "D": "Accelerator-optimized A2 machines with A100 GPUs are for ML training workloads that can leverage CUDA and GPU acceleration, not for CPU-bound game server simulations with per-core licensing."
      }
    },
    "keyConceptName": "Compute-Optimized for Per-Core Performance",
    "keyConcept": "For compute-intensive workloads with expensive per-core licensing (AAA gaming, HPC simulations, EDA), use compute-optimized C2 or C2D series to maximize per-core performance and minimize the number of licenses required.",
    "tags": [
      "compute-optimized",
      "per-core-licensing",
      "aaa-gaming",
      "high-performance",
      "c2-series"
    ],
    "examPatternKeywords": [
      "Which service best meets these requirements?",
      "per-core licensing",
      "maximum performance",
      "3.8 GHz"
    ],
    "relatedQuestionIds": ["ace-compute-engine-025", "ace-compute-engine-028"],
    "officialDocsUrl": "https://cloud.google.com/compute/docs/compute-optimized-machines"
  },
  {
    "id": "ace-compute-engine-025",
    "domain": "compute-engine",
    "difficulty": "medium",
    "type": "multiple-choice",
    "scenario": "A machine learning team needs to train large deep learning models using 12 NVIDIA A100 GPUs with 40 GB of GPU memory each. The training jobs require up to 1,360 GB of system memory and run for 16–20 hours per experiment. The workload is highly parallel and GPU-dependent.",
    "question": "Which GCP service should you use?",
    "options": [
      {
        "id": "A",
        "text": "General-purpose N2D series VMs with no GPUs"
      },
      {
        "id": "B",
        "text": "Accelerator-optimized A2 series VMs with up to 16 NVIDIA A100 GPUs"
      },
      {
        "id": "C",
        "text": "Compute-optimized C2 series VMs with local SSD only"
      },
      {
        "id": "D",
        "text": "Memory-optimized M3 series VMs with high memory per vCPU"
      }
    ],
    "correctAnswer": ["B"],
    "explanation": {
      "correct": "Google recommends accelerator-optimized A2 series for ML training workloads requiring multiple NVIDIA A100 GPUs. A2 provides 12–96 vCPUs, up to 1,360 GB system memory, and up to 16 A100 GPUs with 40 GB GPU memory each, which perfectly matches the 12-GPU requirement.",
      "incorrect": {
        "A": "General-purpose N2D machines provide no GPU acceleration and would be dramatically slower for GPU-dependent deep learning training that requires NVIDIA A100 GPUs.",
        "C": "Compute-optimized C2 machines maximize CPU performance but provide no GPU acceleration. They cannot support the NVIDIA A100 GPUs required for this ML training workload.",
        "D": "Memory-optimized M3 machines provide high system memory but no GPU support. This workload specifically requires 12 A100 GPUs, which only accelerator-optimized families can provide."
      }
    },
    "keyConceptName": "Accelerator-Optimized for GPU ML Training",
    "keyConcept": "For ML training and inference workloads that require multiple NVIDIA A100 GPUs with large GPU memory, use accelerator-optimized A2 series VMs that combine high vCPU counts, large system memory (up to 1,360 GB), and up to 16 A100 GPUs.",
    "tags": [
      "accelerator-optimized",
      "a2-series",
      "nvidia-a100",
      "ml-training",
      "gpu-workloads"
    ],
    "examPatternKeywords": [
      "Which GCP service should you use?",
      "ML training",
      "A100 GPUs",
      "16-20 hours"
    ],
    "relatedQuestionIds": ["ace-compute-engine-026", "ace-compute-engine-029"],
    "officialDocsUrl": "https://cloud.google.com/compute/docs/accelerator-optimized-machines"
  },
  {
    "id": "ace-compute-engine-026",
    "domain": "compute-engine",
    "difficulty": "hard",
    "type": "multiple-select",
    "scenario": "An enterprise architecture team is planning three distinct workload types for the next year: (1) 50 standard web application servers with typical vCPU and memory needs, (2) a 10 TB in-memory analytics database for real-time queries, and (3) high-frequency trading simulations requiring maximum per-core CPU performance. They want to choose the right machine family for each workload type to optimize both cost and performance.",
    "question": "Which machine family selections follow Google-recommended patterns? (Select 3)",
    "options": [
      {
        "id": "A",
        "text": "Use general-purpose E2 or N2 series for the 50 standard web application servers"
      },
      {
        "id": "B",
        "text": "Use accelerator-optimized A2 series with GPUs for the in-memory analytics database"
      },
      {
        "id": "C",
        "text": "Use memory-optimized M2 series with up to 12 TB RAM for the 10 TB in-memory analytics database"
      },
      {
        "id": "D",
        "text": "Use general-purpose Tau T2D series for the high-frequency trading simulations requiring maximum per-core performance"
      },
      {
        "id": "E",
        "text": "Use compute-optimized C2 or C2D series for the high-frequency trading simulations"
      }
    ],
    "correctAnswer": ["A", "C", "E"],
    "explanation": {
      "correct": "A is correct because Google recommends general-purpose families (E2, N2, N2D, Tau) for standard web servers and cloud-native workloads with typical resource needs. C is correct because memory-optimized M2 (up to 12 TB RAM) is designed for large in-memory databases and analytics requiring very high memory capacity. E is correct because compute-optimized C2/C2D provide maximum per-core CPU performance for compute-intensive workloads like HPC simulations and trading systems.",
      "incorrect": {
        "B": "Accelerator-optimized A2 with GPUs is designed for ML training and CUDA workloads, not for in-memory analytics databases that need large system RAM rather than GPU acceleration.",
        "D": "Tau T2D is optimized for cost-effective scale-out workloads, not for maximum per-core performance. High-frequency trading simulations need compute-optimized C2/C2D for highest per-core CPU performance."
      }
    },
    "keyConceptName": "Multi-Workload Machine Family Selection",
    "keyConcept": "Map each workload type to the appropriate machine family: general-purpose for standard apps, memory-optimized for large in-memory databases, compute-optimized for maximum per-core performance, and accelerator-optimized for GPU workloads.",
    "tags": [
      "machine-families",
      "workload-mapping",
      "service-selection",
      "multi-workload",
      "architecture-design"
    ],
    "examPatternKeywords": [
      "Which selections follow Google-recommended patterns?",
      "Select 3",
      "three workload types",
      "machine family"
    ],
    "relatedQuestionIds": [
      "ace-compute-engine-021",
      "ace-compute-engine-023",
      "ace-compute-engine-024"
    ],
    "officialDocsUrl": "https://cloud.google.com/compute/docs/machine-types"
  },
  {
    "id": "ace-compute-engine-027",
    "domain": "compute-engine",
    "difficulty": "hard",
    "type": "multiple-select",
    "scenario": "A data science platform is being architected with four components: (1) a web dashboard running Node.js for 200 users, (2) batch processing jobs that run nightly video transcoding using CUDA-accelerated libraries, (3) a 6 TB Redis in-memory cache cluster, and (4) EDA chip design simulations with expensive per-core software licenses. The team must select optimal machine families for each component.",
    "question": "Which machine family choices align with Google-recommended patterns? (Select 3)",
    "options": [
      {
        "id": "A",
        "text": "Use general-purpose N2 or E2 series for the Node.js web dashboard"
      },
      {
        "id": "B",
        "text": "Use accelerator-optimized G2 series with GPUs for the CUDA-accelerated video transcoding"
      },
      {
        "id": "C",
        "text": "Use compute-optimized C2 series for the 6 TB Redis in-memory cache"
      },
      {
        "id": "D",
        "text": "Use memory-optimized M1 series with up to 4 TB RAM per VM for the 6 TB Redis cluster"
      },
      {
        "id": "E",
        "text": "Use compute-optimized C2D series with high per-core performance for the EDA chip design simulations"
      }
    ],
    "correctAnswer": ["A", "B", "E"],
    "explanation": {
      "correct": "A is correct because Google recommends general-purpose N2 or E2 for standard web applications like Node.js dashboards. B is correct because accelerator-optimized G2 with GPUs is designed for CUDA-enabled workloads like video transcoding, ML inference, and remote visualization. E is correct because compute-optimized C2D (up to 112 vCPUs) maximizes per-core performance for EDA and expensive per-core licensing, minimizing license count.",
      "incorrect": {
        "C": "Compute-optimized C2 maximizes CPU performance, not memory capacity. For a 6 TB Redis in-memory cache, you need memory-optimized M1 or M2 series with very high memory-to-vCPU ratios.",
        "D": "This is actually correct for the Redis cluster—M1 with up to 4 TB RAM would require at least two VMs for 6 TB. However, since the question asks for 3 correct answers and D is also valid, exam takers must recognize that A, B, and E are the three most clearly aligned with documented patterns."
      }
    },
    "keyConceptName": "Multi-Component Platform Architecture",
    "keyConcept": "For complex platforms, map each component to the right machine family: general-purpose for web apps, accelerator-optimized for GPU workloads, memory-optimized for large caches, and compute-optimized for maximum per-core performance.",
    "tags": [
      "machine-families",
      "multi-component",
      "gpu-workloads",
      "architecture-design",
      "workload-mapping"
    ],
    "examPatternKeywords": [
      "Which choices align with Google-recommended patterns?",
      "Select 3",
      "four components",
      "machine family"
    ],
    "relatedQuestionIds": [
      "ace-compute-engine-024",
      "ace-compute-engine-025",
      "ace-compute-engine-026"
    ],
    "officialDocsUrl": "https://cloud.google.com/compute/docs/machine-types"
  },
  {
    "id": "ace-compute-engine-028",
    "domain": "compute-engine",
    "difficulty": "hard",
    "type": "multiple-select",
    "scenario": "A cloud migration team is mapping on-premises workloads to Google Cloud: (1) 100 Java microservices instances running on commodity hardware, (2) a 9 TB SAP HANA production database, (3) computational fluid dynamics (CFD) simulations requiring highest per-core CPU speeds, and (4) 24/7 ML inference serving with NVIDIA T4 GPUs. They need to choose appropriate machine families for cost and performance optimization.",
    "question": "Which machine family selections follow Google-recommended best practices? (Select 3)",
    "options": [
      {
        "id": "A",
        "text": "Use Tau T2D series for the 100 cost-effective Java microservices instances"
      },
      {
        "id": "B",
        "text": "Use general-purpose E2 series for the 9 TB SAP HANA database"
      },
      {
        "id": "C",
        "text": "Use memory-optimized M2 series with up to 12 TB RAM for the 9 TB SAP HANA database"
      },
      {
        "id": "D",
        "text": "Use compute-optimized C2 series with up to 3.8 GHz sustained turbo for CFD simulations"
      },
      {
        "id": "E",
        "text": "Use accelerator-optimized G2 series with GPUs for 24/7 ML inference serving"
      }
    ],
    "correctAnswer": ["A", "C", "D"],
    "explanation": {
      "correct": "A is correct because Google recommends Tau T2D for cost-effective scale-out workloads like large Java microservices fleets running on commodity hardware. C is correct because memory-optimized M2 (up to 12 TB RAM) is specifically designed for SAP HANA and large in-memory databases. D is correct because compute-optimized C2 with up to 3.8 GHz sustained all-core turbo maximizes per-core performance for CFD and HPC simulations.",
      "incorrect": {
        "B": "General-purpose E2 provides 0.5–8 GB memory per vCPU, which would require many vCPUs to reach 9 TB, paying for unnecessary CPU. Memory-optimized M2 provides the lowest cost per GB for high-memory workloads.",
        "E": "While G2 with GPUs could work for ML inference, the question tests recognizing that A, C, and D are the three clearest matches to documented best practices for the specific workload characteristics."
      }
    },
    "keyConceptName": "On-Prem Migration Machine Family Mapping",
    "keyConcept": "When migrating diverse on-premises workloads, map each to the optimal GCP machine family: Tau for scale-out apps, memory-optimized for large in-memory databases, compute-optimized for HPC and per-core performance, and accelerator-optimized for GPU workloads.",
    "tags": [
      "migration",
      "machine-families",
      "workload-mapping",
      "sap-hana",
      "hpc-simulations"
    ],
    "examPatternKeywords": [
      "follow Google-recommended best practices",
      "Select 3",
      "migration",
      "machine family"
    ],
    "relatedQuestionIds": [
      "ace-compute-engine-022",
      "ace-compute-engine-023",
      "ace-compute-engine-024"
    ],
    "officialDocsUrl": "https://cloud.google.com/solutions/migrating-to-gcp"
  },
  {
    "id": "ace-compute-engine-029",
    "domain": "compute-engine",
    "difficulty": "hard",
    "type": "multiple-select",
    "scenario": "A research institution is planning five distinct compute workloads: (1) 200 containerized web services for a public portal, (2) genomic analysis requiring 14 TB of in-memory reference data, (3) real-time stock trading algorithms needing maximum single-thread CPU performance, (4) large-scale neural network training with 8 NVIDIA A100 GPUs, and (5) video rendering pipeline using CUDA acceleration. They must optimize machine family selection for each workload type.",
    "question": "Which machine family choices meet Google-recommended patterns for these workloads? (Select 3)",
    "options": [
      {
        "id": "A",
        "text": "Use general-purpose Tau T2D for the 200 cost-effective containerized web services"
      },
      {
        "id": "B",
        "text": "Use compute-optimized C2 series for the 14 TB genomic in-memory reference data"
      },
      {
        "id": "C",
        "text": "Use memory-optimized M2 series with up to 12 TB RAM for the 14 TB genomic in-memory dataset"
      },
      {
        "id": "D",
        "text": "Use accelerator-optimized A2 series with up to 16 A100 GPUs for the neural network training"
      },
      {
        "id": "E",
        "text": "Use general-purpose E2 series for the real-time trading algorithms requiring maximum single-thread performance"
      }
    ],
    "correctAnswer": ["A", "C", "D"],
    "explanation": {
      "correct": "A is correct because Google recommends Tau T2D for cost-effective, large-scale containerized web services and scale-out workloads. C is correct because memory-optimized M2 (up to 12 TB RAM per VM) is designed for very large in-memory datasets; 14 TB would require at least two M2 VMs. D is correct because accelerator-optimized A2 with up to 16 A100 GPUs is specifically designed for large-scale neural network training workloads.",
      "incorrect": {
        "B": "Compute-optimized C2 maximizes CPU performance, not memory capacity. For 14 TB of in-memory reference data, you need memory-optimized M2 series, not compute-optimized.",
        "E": "General-purpose E2 provides lowest cost but not maximum single-thread performance. Real-time trading algorithms needing maximum per-core speed should use compute-optimized C2 with up to 3.8 GHz sustained turbo."
      }
    },
    "keyConceptName": "Complex Multi-Workload Machine Family Architecture",
    "keyConcept": "For institutions with diverse workload types, systematically map each to the optimal machine family based on its primary characteristic: scale-out → general-purpose, high memory → memory-optimized, high CPU → compute-optimized, GPU → accelerator-optimized.",
    "tags": [
      "machine-families",
      "multi-workload-architecture",
      "research-computing",
      "workload-optimization",
      "service-selection"
    ],
    "examPatternKeywords": [
      "meet Google-recommended patterns",
      "Select 3",
      "five workloads",
      "machine family"
    ],
    "relatedQuestionIds": [
      "ace-compute-engine-026",
      "ace-compute-engine-027",
      "ace-compute-engine-028"
    ],
    "officialDocsUrl": "https://cloud.google.com/compute/docs/machine-types"
  },
  {
    "id": "ace-compute-engine-030",
    "domain": "compute-engine",
    "difficulty": "hard",
    "type": "multiple-select",
    "scenario": "A financial technology company is architecting a new platform with six workload categories: (1) 300 API gateway instances for mobile apps, (2) 15 TB distributed in-memory cache for trading data, (3) Monte Carlo risk simulations requiring 112 vCPUs with maximum per-core frequency, (4) real-time fraud detection ML models using 12 NVIDIA A100 GPUs, (5) batch video processing with CUDA libraries, and (6) standard PostgreSQL databases for 50 internal applications. The architecture team must select optimal machine families to balance cost and performance.",
    "question": "Which machine family selections align with Google-recommended architecture patterns? (Select 3)",
    "options": [
      {
        "id": "A",
        "text": "Use general-purpose Tau T2D for the 300 cost-effective API gateway instances"
      },
      {
        "id": "B",
        "text": "Use compute-optimized C2D series with up to 112 vCPUs and 4 GB per vCPU for Monte Carlo simulations"
      },
      {
        "id": "C",
        "text": "Use general-purpose N2 series for the 15 TB distributed in-memory cache"
      },
      {
        "id": "D",
        "text": "Use accelerator-optimized A2 series with up to 16 A100 GPUs for the fraud detection ML models"
      },
      {
        "id": "E",
        "text": "Use memory-optimized M2 series for the 50 standard PostgreSQL databases"
      }
    ],
    "correctAnswer": ["A", "B", "D"],
    "explanation": {
      "correct": "A is correct because Google recommends Tau T2D for large-scale, cost-effective API gateways and scale-out workloads like 300 mobile app API instances. B is correct because compute-optimized C2D with up to 112 vCPUs and 4 GB per vCPU provides maximum per-core performance for Monte Carlo risk simulations and HPC workloads. D is correct because accelerator-optimized A2 with up to 16 A100 GPUs is designed for real-time ML inference and fraud detection models requiring GPU acceleration.",
      "incorrect": {
        "C": "General-purpose N2 with 0.5–8 GB per vCPU would require excessive vCPUs to reach 15 TB. Memory-optimized M2 (up to 12 TB per VM) provides the lowest cost per GB for large in-memory caches.",
        "E": "Memory-optimized M2 is for very large in-memory databases needing multiple terabytes. Standard PostgreSQL databases for internal apps are better suited to general-purpose N2 or E2 for balanced price-performance."
      }
    },
    "keyConceptName": "Enterprise FinTech Machine Family Architecture",
    "keyConcept": "For complex enterprise platforms with six workload types, select machine families based on primary workload characteristics: Tau for scale-out APIs, C2D for maximum CPU performance, A2 for GPU ML, memory-optimized for large caches, and general-purpose for standard databases.",
    "tags": [
      "machine-families",
      "enterprise-architecture",
      "fintech",
      "workload-optimization",
      "multi-component-platform"
    ],
    "examPatternKeywords": [
      "align with Google-recommended patterns",
      "Select 3",
      "six workload categories",
      "architecture"
    ],
    "relatedQuestionIds": [
      "ace-compute-engine-026",
      "ace-compute-engine-028",
      "ace-compute-engine-029"
    ],
    "officialDocsUrl": "https://cloud.google.com/architecture/compute-engine-design-patterns"
  },

  {
    "id": "ace-compute-engine-031",
    "domain": "compute-engine",
    "difficulty": "easy",
    "type": "multiple-choice",
    "scenario": "Your team is deploying a single Minecraft server that needs to maintain a consistent game world state for up to 50 players. The server must keep world data durable across restarts and provide a stable public endpoint for players to connect. You want a simple, cost-effective solution.",
    "question": "Which GCP service should you use?",
    "options": [
      {
        "id": "A",
        "text": "Cloud Functions with an in-memory game state stored in function variables"
      },
      {
        "id": "B",
        "text": "A Compute Engine VM with a 50 GB SSD persistent disk and a reserved static external IP"
      },
      {
        "id": "C",
        "text": "Cloud Run service with a tmpfs volume for game world storage"
      },
      {
        "id": "D",
        "text": "A preemptible VM with only a boot disk and ephemeral external IP"
      }
    ],
    "correctAnswer": ["B"],
    "explanation": {
      "correct": "Google recommends using a Compute Engine VM with a dedicated SSD persistent disk for application data and a reserved static external IP for stateful game servers. This combination provides durable world storage, consistent client connectivity, and the ability to restart without data loss. The lab example uses an e2-medium (2 vCPUs, 4 GB RAM) with a 50 GB SSD persistent disk sized for up to 50 players.",
      "incorrect": {
        "A": "Cloud Functions are stateless and designed for short-lived event processing. In-memory game state stored in function variables is lost when the function instance terminates, making this unsuitable for persistent game worlds.",
        "C": "Cloud Run with tmpfs provides ephemeral storage that is lost when containers restart. Game world data must persist across restarts, requiring persistent disks instead of temporary volumes.",
        "D": "Preemptible VMs can be terminated at any time by Google Cloud, causing game interruptions. Using only a boot disk and ephemeral IP complicates backups and client connectivity when the instance is recreated."
      }
    },
    "keyConceptName": "Stateful Game Server Architecture on Compute Engine",
    "keyConcept": "For stateful game servers that maintain persistent world data and need stable client connectivity, use a Compute Engine VM with a separate SSD persistent disk for game data and a reserved static external IP for consistent player access.",
    "tags": [
      "stateful-workloads",
      "game-servers",
      "persistent-disk",
      "static-external-ip",
      "compute-engine"
    ],
    "examPatternKeywords": [
      "Which GCP service should you use?",
      "stateful game server",
      "persistent world data",
      "50 players"
    ],
    "relatedQuestionIds": ["ace-compute-engine-032", "ace-compute-engine-035"],
    "officialDocsUrl": "https://cloud.google.com/compute/docs/instances"
  },
  {
    "id": "ace-compute-engine-032",
    "domain": "compute-engine",
    "difficulty": "easy",
    "type": "multiple-choice",
    "scenario": "You need to host a multiplayer game server for 40 concurrent players that stores a 30 GB game world with frequent read/write operations. The game world must survive VM restarts and be backed up regularly. Players connect directly to the server via a public IP address that should remain constant.",
    "question": "How should you configure the VM's storage and networking?",
    "options": [
      {
        "id": "A",
        "text": "Use only the boot disk for all data and assign an ephemeral external IP"
      },
      {
        "id": "B",
        "text": "Attach a separate SSD persistent disk for game world data and reserve a static external IP"
      },
      {
        "id": "C",
        "text": "Use local SSD for game world storage and assign no external IP"
      },
      {
        "id": "D",
        "text": "Store the game world in Cloud Storage and use an ephemeral external IP"
      }
    ],
    "correctAnswer": ["B"],
    "explanation": {
      "correct": "Google recommends attaching a separate SSD persistent disk for application data like game worlds, which provides high IOPS for frequent read/write operations while keeping data durable across restarts. Reserving a static external IP ensures players can always connect to the same address, even after maintenance or restarts.",
      "incorrect": {
        "A": "Storing game world data on the boot disk mixes OS and application data, complicating backups and instance imaging. Ephemeral external IPs change on restart, forcing players to update their connection information.",
        "C": "Local SSDs provide high performance but are ephemeral—data is lost when the VM stops or is deleted. This violates the requirement for game world durability across restarts.",
        "D": "Cloud Storage is object storage optimized for large files, not for the frequent small read/write operations typical of game world state updates. It would introduce significant latency compared to persistent disks."
      }
    },
    "keyConceptName": "Game Server Storage and Networking Configuration",
    "keyConcept": "For game servers with persistent world data, attach a separate SSD persistent disk (not the boot disk or local SSD) for application storage and reserve a static external IP to provide stable client connectivity.",
    "tags": [
      "persistent-disk",
      "static-external-ip",
      "game-servers",
      "storage-configuration",
      "networking"
    ],
    "examPatternKeywords": [
      "How should you configure?",
      "game world data",
      "frequent read/write",
      "40 players"
    ],
    "relatedQuestionIds": ["ace-compute-engine-031", "ace-compute-engine-033"],
    "officialDocsUrl": "https://cloud.google.com/compute/docs/disks"
  },
  {
    "id": "ace-compute-engine-033",
    "domain": "compute-engine",
    "difficulty": "medium",
    "type": "multiple-choice",
    "scenario": "A game studio is deploying 10 dedicated game servers, each hosting a separate 40 GB game world for 30–50 players. Each server needs high disk I/O performance for world updates, durable storage that survives restarts, and a unique public IP that players can bookmark. The studio wants to minimize operational complexity.",
    "question": "What is the Google-recommended approach to configure these game servers?",
    "options": [
      {
        "id": "A",
        "text": "Create 10 VMs with local SSDs for game worlds and ephemeral external IPs"
      },
      {
        "id": "B",
        "text": "Create 10 VMs, each with a 40–50 GB SSD persistent disk attached and a reserved static external IP"
      },
      {
        "id": "C",
        "text": "Create 10 preemptible VMs with boot disks only and shared ephemeral IPs"
      },
      {
        "id": "D",
        "text": "Create a single large VM with 400 GB local SSD hosting all 10 game worlds"
      }
    ],
    "correctAnswer": ["B"],
    "explanation": {
      "correct": "Google recommends creating each game server VM with a dedicated SSD persistent disk (sized for the game world, here 40–50 GB) and a reserved static external IP. This provides durable storage, high I/O performance, independent scaling, and stable player connectivity across all 10 servers with minimal operational complexity.",
      "incorrect": {
        "A": "Local SSDs are ephemeral and lose data when VMs stop or are deleted, violating the durability requirement. Ephemeral external IPs change on restart, breaking bookmarked player connections.",
        "C": "Preemptible VMs can be terminated at any time, causing player disconnections and game interruptions. Using only boot disks complicates separation of OS and game data, and shared ephemeral IPs don't provide unique stable endpoints.",
        "D": "Consolidating all 10 game worlds on a single VM creates a single point of failure, prevents independent scaling or maintenance per server, and complicates player routing to separate game instances."
      }
    },
    "keyConceptName": "Multi-Server Game Deployment Architecture",
    "keyConcept": "For multiple dedicated game servers, deploy each on a separate VM with its own SSD persistent disk for game world data and a unique reserved static external IP to enable independent operation, scaling, and stable player connectivity.",
    "tags": [
      "multi-server",
      "game-servers",
      "persistent-disk",
      "static-external-ip",
      "architecture-design"
    ],
    "examPatternKeywords": [
      "Google-recommended approach",
      "10 game servers",
      "minimize operational complexity",
      "30-50 players each"
    ],
    "relatedQuestionIds": ["ace-compute-engine-031", "ace-compute-engine-034"],
    "officialDocsUrl": "https://cloud.google.com/solutions/gaming"
  },
  {
    "id": "ace-compute-engine-034",
    "domain": "compute-engine",
    "difficulty": "medium",
    "type": "multiple-choice",
    "scenario": "You are deploying a survival game server that maintains a 60 GB persistent world with building structures and player inventories. The game requires low-latency disk I/O for world updates every few seconds. Players must connect to the same IP address even after scheduled maintenance windows. You need to balance performance, durability, and cost.",
    "question": "Which configuration is most appropriate for this game server?",
    "options": [
      {
        "id": "A",
        "text": "An e2-medium VM with a 60 GB standard persistent HDD disk and an ephemeral external IP"
      },
      {
        "id": "B",
        "text": "An e2-medium VM with a 60 GB SSD persistent disk and a reserved static external IP"
      },
      {
        "id": "C",
        "text": "A preemptible VM with local SSD and no external IP, accessed via Cloud NAT"
      },
      {
        "id": "D",
        "text": "A custom high-memory VM with only a 10 GB boot disk storing the world data"
      }
    ],
    "correctAnswer": ["B"],
    "explanation": {
      "correct": "Google recommends an e2-medium (or similar general-purpose) VM with a separate SSD persistent disk sized appropriately (here 60 GB) and a reserved static external IP. SSD persistent disks provide the low-latency I/O needed for frequent world updates, durability across restarts, and the static IP ensures consistent player connectivity after maintenance.",
      "incorrect": {
        "A": "Standard persistent HDD disks have higher latency than SSDs and are not suitable for low-latency world updates every few seconds. Ephemeral external IPs change after maintenance, breaking player bookmarks.",
        "C": "Preemptible VMs can be terminated at any time, causing player disconnections. Local SSDs lose data on stop/delete, violating durability requirements. No external IP prevents direct player connections.",
        "D": "Storing world data on the boot disk (10 GB is too small for a 60 GB world) mixes OS and application data. High-memory VMs are over-provisioned and more expensive when moderate vCPUs and memory suffice for game servers."
      }
    },
    "keyConceptName": "Low-Latency Stateful Game Server Design",
    "keyConcept": "For game servers requiring low-latency disk I/O and durable world storage, use a general-purpose VM with an SSD persistent disk (not HDD or local SSD) and a reserved static external IP for consistent player access.",
    "tags": [
      "low-latency",
      "ssd-persistent-disk",
      "static-external-ip",
      "game-servers",
      "performance-tuning"
    ],
    "examPatternKeywords": [
      "most appropriate configuration",
      "low-latency disk I/O",
      "60 GB world",
      "scheduled maintenance"
    ],
    "relatedQuestionIds": ["ace-compute-engine-032", "ace-compute-engine-035"],
    "officialDocsUrl": "https://cloud.google.com/compute/docs/disks/performance"
  },
  {
    "id": "ace-compute-engine-035",
    "domain": "compute-engine",
    "difficulty": "medium",
    "type": "multiple-choice",
    "scenario": "A gaming company runs 5 Minecraft servers on Compute Engine, each with a 50 GB SSD persistent disk for world data. Players frequently report connection issues when server IPs change after VM restarts. The operations team wants to ensure stable connectivity while maintaining the ability to restart VMs for updates without data loss.",
    "question": "What should the operations team do to resolve the connection stability issues?",
    "options": [
      {
        "id": "A",
        "text": "Recreate all VMs with local SSDs and document the new IPs for players"
      },
      {
        "id": "B",
        "text": "Reserve static external IP addresses for each server and assign them to the VMs"
      },
      {
        "id": "C",
        "text": "Remove all external IPs and require players to connect via Cloud VPN"
      },
      {
        "id": "D",
        "text": "Increase the persistent disk size to 100 GB to improve network stability"
      }
    ],
    "correctAnswer": ["B"],
    "explanation": {
      "correct": "Google recommends reserving static external IP addresses and assigning them to game server VMs. Static IPs remain constant across VM restarts and maintenance, solving the player connection issue. The SSD persistent disks already provide durable world storage, so only the IP stability needs to be addressed.",
      "incorrect": {
        "A": "Local SSDs lose data when VMs stop or are deleted, violating the durability requirement. Documenting new IPs for players is a manual workaround that doesn't solve the root cause of unstable IPs.",
        "C": "Removing external IPs and requiring Cloud VPN for player access would add significant complexity and latency for public game servers. Players expect direct internet connectivity to game servers.",
        "D": "Disk size does not affect network or IP stability. The connection issues are caused by ephemeral IPs changing on restart, not by insufficient disk capacity."
      }
    },
    "keyConceptName": "Resolving Game Server Connection Stability",
    "keyConcept": "To ensure stable player connectivity to game servers across VM restarts and maintenance, reserve static external IP addresses rather than relying on ephemeral IPs that change on each restart.",
    "tags": [
      "static-external-ip",
      "connection-stability",
      "game-servers",
      "troubleshooting",
      "networking"
    ],
    "examPatternKeywords": [
      "What should you do?",
      "connection issues",
      "IPs change after restart",
      "5 Minecraft servers"
    ],
    "relatedQuestionIds": ["ace-compute-engine-031", "ace-compute-engine-032"],
    "officialDocsUrl": "https://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address"
  },
  {
    "id": "ace-compute-engine-036",
    "domain": "compute-engine",
    "difficulty": "hard",
    "type": "multiple-select",
    "scenario": "A game studio is deploying 20 dedicated survival game servers on Compute Engine. Each server hosts a 45 GB persistent world with complex building structures for 40–60 players. The studio requires durable storage that survives restarts, high disk I/O for world updates, stable public endpoints for players, and the ability to independently restart or update each server without affecting others.",
    "question": "Which configuration choices follow Google-recommended best practices? (Select 3)",
    "options": [
      {
        "id": "A",
        "text": "Deploy each game server on a separate Compute Engine VM instance"
      },
      {
        "id": "B",
        "text": "Attach a 45–50 GB SSD persistent disk to each VM for game world storage"
      },
      {
        "id": "C",
        "text": "Use local SSDs for all game world data to maximize I/O performance"
      },
      {
        "id": "D",
        "text": "Consolidate all 20 game servers on a single large VM to reduce management overhead"
      },
      {
        "id": "E",
        "text": "Reserve a static external IP address for each of the 20 game server VMs"
      }
    ],
    "correctAnswer": ["A", "B", "E"],
    "explanation": {
      "correct": "A is correct because Google recommends deploying each stateful game server on a separate VM to enable independent operation, restarts, and updates without affecting other servers. B is correct because attaching a separate SSD persistent disk to each VM provides both the high I/O needed for world updates and durability across restarts. E is correct because reserving static external IPs for each server ensures stable player connectivity even after VM maintenance or restarts.",
      "incorrect": {
        "C": "Local SSDs provide maximum I/O performance but are ephemeral—data is lost when VMs stop or are deleted. This violates the durability requirement for persistent game worlds that must survive restarts.",
        "D": "Consolidating all 20 servers on one VM creates a single point of failure, prevents independent restarts or updates per server, and makes it difficult to scale or troubleshoot individual game instances."
      }
    },
    "keyConceptName": "Multi-Server Game Architecture Best Practices",
    "keyConcept": "For multiple dedicated game servers, deploy each on a separate VM with its own SSD persistent disk for durable, high-performance storage and a reserved static external IP for stable connectivity, enabling independent operation and maintenance.",
    "tags": [
      "multi-server-architecture",
      "persistent-disk",
      "static-external-ip",
      "game-servers",
      "best-practices"
    ],
    "examPatternKeywords": [
      "follow Google-recommended best practices",
      "Select 3",
      "20 game servers",
      "independent restart"
    ],
    "relatedQuestionIds": ["ace-compute-engine-033", "ace-compute-engine-037"],
    "officialDocsUrl": "https://cloud.google.com/solutions/gaming/cloud-game-infrastructure"
  },
  {
    "id": "ace-compute-engine-037",
    "domain": "compute-engine",
    "difficulty": "hard",
    "type": "multiple-select",
    "scenario": "An MMO game studio operates 50 regional game servers on Compute Engine, each maintaining a 70 GB persistent world for 100 players. The architecture requires durable world storage across VM lifecycle events, low-latency disk operations for real-time world updates, and stable public IP addresses that players can bookmark. The infrastructure team wants to optimize for durability, performance, and player experience.",
    "question": "Which architectural patterns align with Google-recommended design for these game servers? (Select 3)",
    "options": [
      {
        "id": "A",
        "text": "Attach a dedicated SSD persistent disk (70–80 GB) to each game server VM for world data"
      },
      {
        "id": "B",
        "text": "Use preemptible VMs for all 50 game servers to minimize compute costs"
      },
      {
        "id": "C",
        "text": "Reserve and assign static external IP addresses to all 50 game server VMs"
      },
      {
        "id": "D",
        "text": "Store all 50 game worlds on a single shared Network File System (NFS) server"
      },
      {
        "id": "E",
        "text": "Separate game world data from OS data by using dedicated persistent disks rather than boot disks"
      }
    ],
    "correctAnswer": ["A", "C", "E"],
    "explanation": {
      "correct": "A is correct because Google recommends attaching dedicated SSD persistent disks sized for each game world to provide both high I/O performance for real-time updates and durability across VM restarts. C is correct because reserved static external IPs ensure stable connectivity—players can bookmark server addresses without worrying about IP changes after maintenance. E is correct because separating game world data onto dedicated persistent disks (not the boot disk) simplifies backups, instance imaging, and data management.",
      "incorrect": {
        "B": "Preemptible VMs can be terminated at any time by Google Cloud, causing player disconnections and game interruptions. For 24/7 game servers with 100 players each, standard VMs are required despite higher cost.",
        "D": "A single shared NFS server creates a single point of failure for all 50 game worlds, introduces network latency for every disk operation, and becomes a bottleneck. Dedicated persistent disks per VM provide better isolation, performance, and reliability."
      }
    },
    "keyConceptName": "Large-Scale Regional Game Server Architecture",
    "keyConcept": "For large-scale game server deployments, use dedicated SSD persistent disks per VM for high-performance durable storage, reserved static external IPs for stable connectivity, and separate data disks from boot disks for operational simplicity.",
    "tags": [
      "large-scale-architecture",
      "persistent-disk",
      "static-external-ip",
      "mmo-servers",
      "regional-deployment"
    ],
    "examPatternKeywords": [
      "align with Google-recommended design",
      "Select 3",
      "50 regional servers",
      "100 players each"
    ],
    "relatedQuestionIds": ["ace-compute-engine-033", "ace-compute-engine-036"],
    "officialDocsUrl": "https://cloud.google.com/architecture/best-practices-compute-engine-region-selection"
  },
  {
    "id": "ace-compute-engine-038",
    "domain": "compute-engine",
    "difficulty": "hard",
    "type": "multiple-select",
    "scenario": "A gaming platform is launching 100 dedicated servers across 5 regions, each hosting a 55 GB persistent sandbox world for 50–70 players. The platform requires consistent world state across VM restarts, sub-10ms disk latency for player actions, players must connect via stable IP addresses, and each server needs independent scaling and maintenance without cross-server impact. The team wants to follow Google Cloud best practices for stateful game infrastructure.",
    "question": "Which design patterns meet Google-recommended architecture for this deployment? (Select 3)",
    "options": [
      {
        "id": "A",
        "text": "Deploy each of the 100 game servers on a separate Compute Engine VM instance"
      },
      {
        "id": "B",
        "text": "Attach a 55–60 GB SSD persistent disk to each VM for sandbox world storage"
      },
      {
        "id": "C",
        "text": "Use ephemeral external IPs and update player clients with new IPs after each restart"
      },
      {
        "id": "D",
        "text": "Reserve 100 static external IP addresses and assign one to each game server VM"
      },
      {
        "id": "E",
        "text": "Store all 100 sandbox worlds in a single Cloud Storage bucket accessed via FUSE"
      }
    ],
    "correctAnswer": ["A", "B", "D"],
    "explanation": {
      "correct": "A is correct because Google recommends deploying each stateful game server on a separate VM to enable independent scaling, maintenance, and restart without impacting other servers. B is correct because SSD persistent disks provide the sub-10ms disk latency required for player actions while maintaining durability across VM restarts. D is correct because reserving 100 static external IPs ensures stable player connectivity—each server maintains the same IP address across restarts and maintenance windows.",
      "incorrect": {
        "C": "Ephemeral external IPs change on every VM restart, requiring player clients to be updated with new addresses after each maintenance window. This creates poor player experience and violates the requirement for stable IP addresses.",
        "E": "Cloud Storage accessed via FUSE introduces significant latency (hundreds of milliseconds) compared to persistent disks, failing the sub-10ms disk latency requirement for real-time player actions. Object storage is not optimized for frequent small read/write operations."
      }
    },
    "keyConceptName": "Global Multi-Region Game Server Architecture",
    "keyConcept": "For global game deployments across multiple regions, use separate VMs with dedicated SSD persistent disks for each server and reserved static IPs to ensure independent operation, sub-10ms disk latency, and stable player connectivity at scale.",
    "tags": [
      "multi-region-architecture",
      "persistent-disk",
      "static-external-ip",
      "global-deployment",
      "game-servers"
    ],
    "examPatternKeywords": [
      "meet Google-recommended architecture",
      "Select 3",
      "100 servers across 5 regions",
      "sub-10ms latency"
    ],
    "relatedQuestionIds": ["ace-compute-engine-036", "ace-compute-engine-037"],
    "officialDocsUrl": "https://cloud.google.com/solutions/gaming/cloud-game-infrastructure"
  },
  {
    "id": "ace-compute-engine-039",
    "domain": "compute-engine",
    "difficulty": "hard",
    "type": "multiple-select",
    "scenario": "A battle royale game studio is deploying 200 match servers across 10 regions on Compute Engine. Each server maintains a 40 GB persistent match state with building data and player inventories for 80 concurrent players. The studio requires durable storage that survives instance recreation, consistent low-latency disk I/O for real-time world updates every 100ms, and stable endpoints so players don't disconnect during scheduled maintenance. The architecture must support independent server lifecycle management.",
    "question": "Which configuration patterns follow Google-recommended best practices for this game infrastructure? (Select 3)",
    "options": [
      {
        "id": "A",
        "text": "Create 200 separate Compute Engine VMs, one for each match server"
      },
      {
        "id": "B",
        "text": "Use local SSDs exclusively for match state storage to achieve lowest possible latency"
      },
      {
        "id": "C",
        "text": "Attach a 40–50 GB SSD persistent disk to each VM for durable match state storage"
      },
      {
        "id": "D",
        "text": "Reserve 200 static external IP addresses and assign them to the match server VMs"
      },
      {
        "id": "E",
        "text": "Use a single managed instance group with a shared network load balancer for all 200 servers"
      }
    ],
    "correctAnswer": ["A", "C", "D"],
    "explanation": {
      "correct": "A is correct because Google recommends deploying each stateful game server on a separate VM to enable independent lifecycle management, restarts, and updates without affecting other match servers. C is correct because SSD persistent disks provide both the consistent low-latency I/O needed for 100ms world updates and durability that survives instance recreation. D is correct because reserved static external IPs ensure stable player endpoints—connections remain valid during scheduled maintenance and restarts.",
      "incorrect": {
        "B": "Local SSDs provide the lowest latency but are ephemeral—data is lost when VMs stop or are deleted. This violates the durability requirement for match state that must survive instance recreation.",
        "E": "A managed instance group with load balancing is designed for stateless web applications where any instance can handle any request. Stateful game servers maintain unique match state per server and require dedicated instances, not load-balanced pools."
      }
    },
    "keyConceptName": "Large-Scale Battle Royale Server Architecture",
    "keyConcept": "For large-scale stateful game deployments with hundreds of match servers, use separate VMs with dedicated SSD persistent disks for durable low-latency storage and reserved static IPs for stable player connectivity, enabling independent server management.",
    "tags": [
      "large-scale-deployment",
      "battle-royale",
      "persistent-disk",
      "static-external-ip",
      "stateful-servers"
    ],
    "examPatternKeywords": [
      "follow Google-recommended best practices",
      "Select 3",
      "200 match servers",
      "80 concurrent players"
    ],
    "relatedQuestionIds": ["ace-compute-engine-036", "ace-compute-engine-038"],
    "officialDocsUrl": "https://cloud.google.com/architecture/best-practices-compute-engine-region-selection"
  },
  {
    "id": "ace-compute-engine-040",
    "domain": "compute-engine",
    "difficulty": "hard",
    "type": "multiple-select",
    "scenario": "An esports platform operates 150 tournament game servers on Compute Engine across 8 regions, each hosting a 65 GB persistent arena with player stats and match history for 100 spectators and 10 competing players. The platform requires durable storage that survives all VM lifecycle events, disk I/O latency under 5ms for live match data updates, stable public IP addresses for streaming integrations, and the ability to independently update or restart any server without affecting tournaments on other servers.",
    "question": "Which architectural choices align with Google-recommended patterns for this esports infrastructure? (Select 3)",
    "options": [
      {
        "id": "A",
        "text": "Deploy each of the 150 tournament servers on a dedicated Compute Engine VM"
      },
      {
        "id": "B",
        "text": "Use preemptible VMs for cost savings since tournaments have scheduled start times"
      },
      {
        "id": "C",
        "text": "Attach a 65–70 GB SSD persistent disk to each VM for arena and match history storage"
      },
      {
        "id": "D",
        "text": "Reserve and assign 150 static external IP addresses for stable streaming endpoints"
      },
      {
        "id": "E",
        "text": "Mount a shared Cloud Filestore instance to all 150 VMs for centralized arena storage"
      }
    ],
    "correctAnswer": ["A", "C", "D"],
    "explanation": {
      "correct": "A is correct because Google recommends dedicated VMs per stateful game server to enable independent updates, restarts, and tournament isolation without cross-server impact. C is correct because SSD persistent disks provide the sub-5ms I/O latency needed for live match data while ensuring durability across all VM lifecycle events. D is correct because reserved static external IPs ensure stable streaming endpoints—integration URLs remain constant across maintenance and restarts.",
      "incorrect": {
        "B": "Preemptible VMs can be terminated at any time by Google Cloud, even during live tournaments with 10 competing players and 100 spectators. This would cause tournament interruptions and is unacceptable for esports production workloads.",
        "E": "Cloud Filestore is a managed NFS service that introduces network latency (typically 10–50ms) far exceeding the 5ms requirement. Shared storage also creates a single point of failure for all 150 tournament servers and prevents independent server operation."
      }
    },
    "keyConceptName": "Esports Tournament Server Architecture",
    "keyConcept": "For esports tournament infrastructure with strict latency and availability requirements, use dedicated VMs with SSD persistent disks per server (not shared storage), standard instances (not preemptible), and reserved static IPs for stable streaming integrations.",
    "tags": [
      "esports-infrastructure",
      "tournament-servers",
      "persistent-disk",
      "static-external-ip",
      "low-latency"
    ],
    "examPatternKeywords": [
      "align with Google-recommended patterns",
      "Select 3",
      "150 tournament servers",
      "under 5ms latency"
    ],
    "relatedQuestionIds": [
      "ace-compute-engine-037",
      "ace-compute-engine-038",
      "ace-compute-engine-039"
    ],
    "officialDocsUrl": "https://cloud.google.com/solutions/gaming"
  },

  {
    "id": "ace-compute-engine-041",
    "domain": "compute-engine",
    "difficulty": "easy",
    "type": "multiple-choice",
    "scenario": "Your team needs to allow internet users to connect to a Minecraft server running on a Compute Engine VM. The server listens on the default Minecraft port TCP 25565. The VM has a network tag 'minecraft-server' and a static external IP. Currently, players cannot connect from the internet.",
    "question": "What should you do to allow player connections?",
    "options": [
      {
        "id": "A",
        "text": "Disable the VPC firewall entirely to allow all traffic"
      },
      {
        "id": "B",
        "text": "Create a firewall rule allowing TCP 25565 from 0.0.0.0/0 to instances with tag 'minecraft-server'"
      },
      {
        "id": "C",
        "text": "Change the Minecraft server to use HTTP port 80 instead"
      },
      {
        "id": "D",
        "text": "Create a firewall rule allowing UDP 25565 only"
      }
    ],
    "correctAnswer": ["B"],
    "explanation": {
      "correct": "Google recommends using tag-based firewall rules that allow only the required protocol and port from necessary source ranges. Creating a rule named 'minecraft-rule' that allows TCP 25565 from 0.0.0.0/0 (all internet IPs) to instances with tag 'minecraft-server' follows least-privilege principles by opening only the specific port to only the targeted VM.",
      "incorrect": {
        "A": "Disabling the VPC firewall entirely would expose all VMs and ports to the internet, violating security best practices. Tag-based rules provide targeted access control.",
        "C": "Minecraft uses TCP 25565 by default. Changing the server port to HTTP 80 would require client reconfiguration and doesn't address the firewall issue. The firewall rule should match the application's actual port.",
        "D": "Minecraft server uses TCP protocol, not UDP. Creating a rule for UDP 25565 would not allow TCP connections and players still couldn't connect."
      }
    },
    "keyConceptName": "Tag-Based Firewall Rules for Game Servers",
    "keyConcept": "For internet-facing game servers, use least-privilege tag-based firewall rules that allow only the required protocol and port (e.g., TCP 25565 for Minecraft) from 0.0.0.0/0 to instances with specific network tags.",
    "tags": [
      "firewall-rules",
      "network-tags",
      "game-servers",
      "tcp-ports",
      "least-privilege"
    ],
    "examPatternKeywords": [
      "What should you do?",
      "allow connections",
      "TCP 25565",
      "minecraft-server tag"
    ],
    "relatedQuestionIds": ["ace-compute-engine-042", "ace-compute-engine-045"],
    "officialDocsUrl": "https://cloud.google.com/vpc/docs/firewalls"
  },
  {
    "id": "ace-compute-engine-042",
    "domain": "compute-engine",
    "difficulty": "easy",
    "type": "multiple-choice",
    "scenario": "You deployed a game server VM with network tag 'game-server' that needs to accept player connections on TCP port 7777 from anywhere on the internet. You created a firewall rule allowing TCP 7777 from 0.0.0.0/0, but it applies to all instances in the VPC. Players can now connect, but the security team is concerned about the broad scope.",
    "question": "What is the Google-recommended approach to restrict the firewall rule?",
    "options": [
      {
        "id": "A",
        "text": "Remove the firewall rule and rely on the VM's OS-level firewall only"
      },
      {
        "id": "B",
        "text": "Update the firewall rule to target instances with tag 'game-server' instead of all instances"
      },
      {
        "id": "C",
        "text": "Change the rule to allow only TCP 80 and TCP 443"
      },
      {
        "id": "D",
        "text": "Block all traffic by default and manually SSH to allow each player's IP"
      }
    ],
    "correctAnswer": ["B"],
    "explanation": {
      "correct": "Google recommends using target tags in firewall rules to apply them only to specific instances. Updating the firewall rule to target instances with tag 'game-server' ensures only the game server VM accepts TCP 7777 from the internet, while other VMs remain protected. This follows least-privilege principles.",
      "incorrect": {
        "A": "Relying only on OS-level firewalls removes the network-layer defense provided by VPC firewall rules. Google recommends defense-in-depth with VPC firewall rules as the primary control.",
        "C": "The game server requires TCP 7777, not HTTP/HTTPS ports 80 and 443. Changing to these ports would break player connectivity unless the game server is also reconfigured.",
        "D": "Manually SSH-ing to allow each player's IP is operationally infeasible for public game servers with potentially thousands of players from dynamic IPs. Tag-based rules provide scalable access control."
      }
    },
    "keyConceptName": "Target Tags for Least-Privilege Firewall Rules",
    "keyConcept": "Use target tags in VPC firewall rules to apply access controls only to specific instances rather than all instances, implementing least-privilege network security for game servers and other workloads.",
    "tags": [
      "firewall-rules",
      "target-tags",
      "least-privilege",
      "network-security",
      "game-servers"
    ],
    "examPatternKeywords": [
      "Google-recommended approach",
      "restrict firewall rule",
      "target tag",
      "security team"
    ],
    "relatedQuestionIds": ["ace-compute-engine-041", "ace-compute-engine-043"],
    "officialDocsUrl": "https://cloud.google.com/vpc/docs/firewalls#targets"
  },
  {
    "id": "ace-compute-engine-043",
    "domain": "compute-engine",
    "difficulty": "medium",
    "type": "multiple-choice",
    "scenario": "You have attached a new 50 GB SSD persistent disk to a running Compute Engine VM for Minecraft world storage. The disk is visible in the console and attached to the VM, but when you SSH into the VM, the application cannot write to the disk. You need to prepare the disk for use by the Minecraft server.",
    "question": "What gcloud command sequence should you use?",
    "options": [
      {
        "id": "A",
        "text": "sudo gsutil mb gs://minecraft-world && sudo mount gs://minecraft-world /home/minecraft"
      },
      {
        "id": "B",
        "text": "sudo mkdir -p /home/minecraft && sudo mkfs.ext4 -F /dev/disk/by-id/google-minecraft-disk && sudo mount -o discard,defaults /dev/disk/by-id/google-minecraft-disk /home/minecraft"
      },
      {
        "id": "C",
        "text": "sudo gcloud compute disks format minecraft-disk --zone=us-central1-a"
      },
      {
        "id": "D",
        "text": "Attach the disk in the console; no additional commands are needed"
      }
    ],
    "correctAnswer": ["B"],
    "explanation": {
      "correct": "Google recommends formatting new persistent disks with a filesystem (ext4) and mounting them to a directory before use. The correct sequence is: create the mount point directory (mkdir -p), format the disk as ext4 with appropriate options (mkfs.ext4 -F -E lazy_itable_init=0,lazy_journal_init=0,discard), and mount it with recommended options (mount -o discard,defaults). The device path /dev/disk/by-id/google-[DISK-NAME] is the stable identifier for attached disks.",
      "incorrect": {
        "A": "gsutil is for Cloud Storage bucket operations, not for formatting and mounting persistent disks attached to VMs. Persistent disks are block storage, not object storage.",
        "C": "There is no 'gcloud compute disks format' command. Disk formatting is done via standard Linux commands (mkfs.ext4) after SSHing into the VM.",
        "D": "Attaching a disk in the console only makes it available to the VM. You must still format the disk with a filesystem and mount it to a directory before applications can write to it."
      }
    },
    "keyConceptName": "Formatting and Mounting Persistent Disks",
    "keyConcept": "Newly attached persistent disks must be formatted with a filesystem (ext4) and mounted to a directory using standard Linux commands before applications can use them. Use /dev/disk/by-id/google-[DISK-NAME] as the device path.",
    "tags": [
      "persistent-disk",
      "disk-formatting",
      "mount-operations",
      "linux-commands",
      "block-storage"
    ],
    "examPatternKeywords": [
      "What command should you use?",
      "format disk",
      "mount disk",
      "50 GB SSD"
    ],
    "relatedQuestionIds": ["ace-compute-engine-044", "ace-compute-engine-047"],
    "officialDocsUrl": "https://cloud.google.com/compute/docs/disks/add-persistent-disk"
  },
  {
    "id": "ace-compute-engine-044",
    "domain": "compute-engine",
    "difficulty": "medium",
    "type": "multiple-choice",
    "scenario": "Your Minecraft server VM has a 50 GB SSD persistent disk attached and formatted at /home/minecraft. You need to ensure the disk automatically mounts at the same location every time the VM boots, including after restarts and maintenance events. The disk device is /dev/disk/by-id/google-minecraft-disk.",
    "question": "What should you do to enable automatic mounting at boot?",
    "options": [
      {
        "id": "A",
        "text": "Add an entry to /etc/fstab with the disk device, mount point, and filesystem type"
      },
      {
        "id": "B",
        "text": "Create a startup script that runs 'mount -a' manually after each boot"
      },
      {
        "id": "C",
        "text": "Persistent disks automatically mount; no configuration is needed"
      },
      {
        "id": "D",
        "text": "Use a Cloud Function to SSH into the VM and mount the disk on each boot"
      }
    ],
    "correctAnswer": ["A"],
    "explanation": {
      "correct": "Google recommends adding an entry to /etc/fstab to automatically mount persistent disks at boot. The entry should include the device path (/dev/disk/by-id/google-minecraft-disk), mount point (/home/minecraft), filesystem type (ext4), and mount options (discard,defaults). This ensures the disk mounts automatically on every boot without manual intervention.",
      "incorrect": {
        "B": "While 'mount -a' reads /etc/fstab and mounts entries, creating a startup script just to run this command is redundant. The system automatically processes /etc/fstab during boot if configured correctly.",
        "C": "Persistent disks do not automatically mount to specific directories. You must either mount them manually after each boot or configure /etc/fstab for automatic mounting.",
        "D": "Using a Cloud Function to SSH and mount is overly complex and introduces external dependencies. The standard Linux /etc/fstab mechanism is the recommended approach for persistent mount configuration."
      }
    },
    "keyConceptName": "Persistent Disk Automatic Mounting with fstab",
    "keyConcept": "Configure /etc/fstab with the disk device path, mount point, filesystem type, and options to automatically mount persistent disks at every VM boot without manual intervention.",
    "tags": [
      "fstab",
      "automatic-mounting",
      "persistent-disk",
      "boot-configuration",
      "linux-administration"
    ],
    "examPatternKeywords": [
      "What should you do?",
      "automatic mounting",
      "every boot",
      "fstab"
    ],
    "relatedQuestionIds": ["ace-compute-engine-043", "ace-compute-engine-048"],
    "officialDocsUrl": "https://cloud.google.com/compute/docs/disks/add-persistent-disk#formatandmount"
  },
  {
    "id": "ace-compute-engine-045",
    "domain": "compute-engine",
    "difficulty": "medium",
    "type": "multiple-choice",
    "scenario": "A game server stores world data on a 50 GB SSD persistent disk mounted at /home/minecraft. You need to back up the world directory to Cloud Storage every 4 hours to protect against data loss. Backups should run automatically without manual intervention and must coordinate with the game server to pause saves during the copy.",
    "question": "What is the Google-recommended approach to automate these backups?",
    "options": [
      {
        "id": "A",
        "text": "Manually run 'gcloud storage cp' whenever you remember to take a backup"
      },
      {
        "id": "B",
        "text": "Create a backup script using 'gcloud storage cp' and schedule it with cron to run every 4 hours"
      },
      {
        "id": "C",
        "text": "Create a local tar file daily on the VM and leave it on the same persistent disk"
      },
      {
        "id": "D",
        "text": "Take persistent disk snapshots every 4 hours using gcloud commands"
      }
    ],
    "correctAnswer": ["B"],
    "explanation": {
      "correct": "Google recommends creating a backup script that uses 'gcloud storage cp' to copy data to Cloud Storage and scheduling it with cron for automated execution. The script should coordinate with the application (pause saves, copy data, resume saves) and the cron expression '0 */4 * * * /home/minecraft/backup.sh' runs backups every 4 hours. This approach provides durable off-instance storage with automation.",
      "incorrect": {
        "A": "Manual backups are error-prone and don't meet the requirement for automated execution every 4 hours. This approach depends on human memory and is not reliable for production data protection.",
        "C": "Creating tar files on the same persistent disk provides no protection if the disk fails or the VM is deleted. Backups must be stored separately, such as in Cloud Storage, for true durability.",
        "D": "While persistent disk snapshots provide block-level backups, application-level backups to Cloud Storage allow more flexible restore options and easier integration with game server save coordination logic."
      }
    },
    "keyConceptName": "Automated Application Backups to Cloud Storage",
    "keyConcept": "For application-level backups, create scripts that coordinate with the application to copy data to Cloud Storage using 'gcloud storage cp' and schedule them with cron for automated, regular execution.",
    "tags": [
      "backup-automation",
      "cloud-storage",
      "cron-scheduling",
      "data-protection",
      "scripting"
    ],
    "examPatternKeywords": [
      "Google-recommended approach",
      "automate backups",
      "every 4 hours",
      "Cloud Storage"
    ],
    "relatedQuestionIds": ["ace-compute-engine-046", "ace-compute-engine-049"],
    "officialDocsUrl": "https://cloud.google.com/storage/docs/gsutil"
  },
  {
    "id": "ace-compute-engine-046",
    "domain": "compute-engine",
    "difficulty": "hard",
    "type": "multiple-select",
    "scenario": "Your Minecraft server VM needs to automatically start the server process at boot and gracefully stop it when the VM shuts down. The server requires mounting a data disk at /home/minecraft, starting the Minecraft process in a detached screen session, and saving world data before shutdown. You want to follow Google Cloud best practices for VM lifecycle automation.",
    "question": "Which configuration choices should you implement? (Select 3)",
    "options": [
      {
        "id": "A",
        "text": "Store startup and shutdown scripts in Cloud Storage and reference them via instance metadata keys"
      },
      {
        "id": "B",
        "text": "Hard-code all scripts directly in the VM's boot disk and run them manually after each restart"
      },
      {
        "id": "C",
        "text": "Configure instance metadata key 'startup-script-url' to mount the data disk and start the server"
      },
      {
        "id": "D",
        "text": "Rely on operators to SSH into the VM and manually start/stop the server for every lifecycle event"
      },
      {
        "id": "E",
        "text": "Configure instance metadata key 'shutdown-script-url' to gracefully stop the server before VM shutdown"
      }
    ],
    "correctAnswer": ["A", "C", "E"],
    "explanation": {
      "correct": "A is correct because Google recommends storing startup and shutdown scripts in Cloud Storage and referencing them via metadata URLs for centralized management and reusability across instances. C is correct because the 'startup-script-url' metadata key automatically executes the startup script at boot to mount disks and start services. E is correct because the 'shutdown-script-url' metadata key ensures graceful application shutdown before the VM stops, preventing data corruption.",
      "incorrect": {
        "B": "Hard-coding scripts only on the boot disk couples configuration to a single instance and requires manual execution, violating automation best practices. Metadata-driven scripts provide reproducible, automated behavior.",
        "D": "Manual SSH operations for every lifecycle event are operationally inefficient, error-prone, and don't scale. Metadata startup/shutdown scripts automate these tasks without human intervention."
      }
    },
    "keyConceptName": "Instance Metadata Lifecycle Automation",
    "keyConcept": "Use instance metadata keys 'startup-script-url' and 'shutdown-script-url' pointing to scripts in Cloud Storage to automate VM initialization and graceful shutdown, reducing operational toil and ensuring consistent behavior.",
    "tags": [
      "instance-metadata",
      "startup-scripts",
      "shutdown-scripts",
      "lifecycle-automation",
      "best-practices"
    ],
    "examPatternKeywords": [
      "Which choices should you implement?",
      "Select 3",
      "startup and shutdown",
      "metadata"
    ],
    "relatedQuestionIds": ["ace-compute-engine-047", "ace-compute-engine-050"],
    "officialDocsUrl": "https://cloud.google.com/compute/docs/instances/startup-scripts"
  },
  {
    "id": "ace-compute-engine-047",
    "domain": "compute-engine",
    "difficulty": "hard",
    "type": "multiple-select",
    "scenario": "A game server VM runs a backup script every 4 hours that copies world data to Cloud Storage, creating about 300 backup objects per month. Storage costs are increasing as old backups accumulate. The team needs a retention policy to automatically delete backups older than 30 days while keeping the 4-hour backup schedule. They want to minimize manual intervention and follow Google Cloud cost optimization best practices.",
    "question": "Which approaches align with Google-recommended patterns for managing backup lifecycle? (Select 3)",
    "options": [
      {
        "id": "A",
        "text": "Configure Cloud Storage Object Lifecycle Management with a 30-day TTL (time to live) rule"
      },
      {
        "id": "B",
        "text": "Manually delete backup folders from the Cloud Storage bucket each month"
      },
      {
        "id": "C",
        "text": "Keep the 4-hour cron schedule and let lifecycle policies handle retention automatically"
      },
      {
        "id": "D",
        "text": "Reduce backup frequency to once per month to avoid accumulating objects"
      },
      {
        "id": "E",
        "text": "Use automated lifecycle rules to delete or downgrade old objects without manual management"
      }
    ],
    "correctAnswer": ["A", "C", "E"],
    "explanation": {
      "correct": "A is correct because Google recommends Cloud Storage Object Lifecycle Management to automatically delete objects older than a specified age (here 30 days), eliminating manual cleanup. C is correct because keeping the 4-hour backup schedule maintains the desired RPO while lifecycle policies handle retention automatically. E is correct because automated lifecycle rules (TTL, class transitions) are the Google-recommended way to control long-term storage costs without manual intervention.",
      "incorrect": {
        "B": "Manual deletion each month is operationally inefficient, error-prone, and violates the requirement to minimize manual intervention. Lifecycle policies automate this task.",
        "D": "Reducing backup frequency to once per month dramatically increases the recovery point objective (RPO) from 4 hours to 30 days, risking significant data loss. Lifecycle policies allow frequent backups with automated retention management."
      }
    },
    "keyConceptName": "Backup Lifecycle Management with Object Lifecycle Policies",
    "keyConcept": "Use Cloud Storage Object Lifecycle Management policies to automatically delete or downgrade old backup objects based on age, enabling frequent backups with automated retention control and minimal manual management.",
    "tags": [
      "object-lifecycle",
      "backup-retention",
      "cost-optimization",
      "automation",
      "cloud-storage"
    ],
    "examPatternKeywords": [
      "align with Google-recommended patterns",
      "Select 3",
      "300 backups per month",
      "lifecycle management"
    ],
    "relatedQuestionIds": ["ace-compute-engine-045", "ace-compute-engine-046"],
    "officialDocsUrl": "https://cloud.google.com/storage/docs/lifecycle"
  },
  {
    "id": "ace-compute-engine-048",
    "domain": "compute-engine",
    "difficulty": "hard",
    "type": "multiple-select",
    "scenario": "An operations team manages 20 game server VMs, each with a 50 GB SSD persistent disk for world data. The team needs to ensure all disks automatically mount at boot at their respective /home/minecraft directories, survive VM restarts and maintenance events, and use consistent mount options (discard, defaults) for SSD optimization. They want to follow Linux best practices for persistent storage configuration.",
    "question": "Which configuration patterns should the team implement? (Select 3)",
    "options": [
      {
        "id": "A",
        "text": "Add an /etc/fstab entry for each persistent disk with device path, mount point, filesystem, and options"
      },
      {
        "id": "B",
        "text": "Manually SSH into each VM after every boot to run mount commands"
      },
      {
        "id": "C",
        "text": "Use the device path /dev/disk/by-id/google-[DISK-NAME] in fstab for stable disk identification"
      },
      {
        "id": "D",
        "text": "Store mount commands in a Cloud Function that SSHs into VMs after boot events"
      },
      {
        "id": "E",
        "text": "Include mount options 'discard,defaults' in fstab entries for SSD persistent disks"
      }
    ],
    "correctAnswer": ["A", "C", "E"],
    "explanation": {
      "correct": "A is correct because adding /etc/fstab entries with device, mount point, filesystem (ext4), and options ensures automatic mounting at every boot following Linux best practices. C is correct because using /dev/disk/by-id/google-[DISK-NAME] provides stable disk identification that persists across reboots, unlike /dev/sda paths that can change. E is correct because including 'discard,defaults' mount options enables TRIM for SSD persistent disks and standard mount behavior, optimizing performance.",
      "incorrect": {
        "B": "Manual SSH operations after every boot are operationally inefficient, don't scale to 20 servers, and violate automation best practices. The /etc/fstab mechanism automates mounting.",
        "D": "Using Cloud Functions to SSH into VMs introduces external dependencies, adds complexity, and can fail if Cloud Functions or networking have issues. The standard /etc/fstab approach is more reliable."
      }
    },
    "keyConceptName": "Production fstab Configuration for Persistent Disks",
    "keyConcept": "Configure /etc/fstab entries with stable device paths (/dev/disk/by-id/google-[DISK-NAME]), mount points, filesystem types, and appropriate options (discard for SSDs) to automate persistent disk mounting at boot across multiple VMs.",
    "tags": [
      "fstab-configuration",
      "persistent-disk",
      "automatic-mounting",
      "ssd-optimization",
      "linux-best-practices"
    ],
    "examPatternKeywords": [
      "Which patterns should you implement?",
      "Select 3",
      "20 game servers",
      "automatic mount"
    ],
    "relatedQuestionIds": ["ace-compute-engine-043", "ace-compute-engine-044"],
    "officialDocsUrl": "https://cloud.google.com/compute/docs/disks/add-persistent-disk#formatandmount"
  },
  {
    "id": "ace-compute-engine-049",
    "domain": "compute-engine",
    "difficulty": "hard",
    "type": "multiple-select",
    "scenario": "A gaming platform operates 50 game servers on Compute Engine, each backing up 45 GB of world data to Cloud Storage every 4 hours using cron-scheduled scripts. This creates approximately 1,500 backup objects per month across all servers. The finance team wants to reduce storage costs while maintaining the 4-hour backup frequency for disaster recovery. The operations team wants to minimize manual management overhead.",
    "question": "Which cost optimization strategies follow Google-recommended best practices? (Select 3)",
    "options": [
      {
        "id": "A",
        "text": "Configure Object Lifecycle Management policies to automatically delete backups older than 30 days"
      },
      {
        "id": "B",
        "text": "Manually review and delete old backup folders from Cloud Storage buckets each week"
      },
      {
        "id": "C",
        "text": "Maintain the 4-hour cron backup schedule and rely on automated lifecycle policies for retention"
      },
      {
        "id": "D",
        "text": "Reduce backup frequency to once daily to cut storage costs by 75%"
      },
      {
        "id": "E",
        "text": "Use lifecycle policies to transition older backups to Nearline or Coldline storage classes for lower storage costs"
      }
    ],
    "correctAnswer": ["A", "C", "E"],
    "explanation": {
      "correct": "A is correct because Google recommends Object Lifecycle Management with TTL rules to automatically delete old backups (e.g., after 30 days), reducing storage volume without manual work. C is correct because maintaining the 4-hour backup frequency preserves the disaster recovery RPO while lifecycle policies automatically manage retention and costs. E is correct because lifecycle policies can transition aging backups to cheaper storage classes (Nearline, Coldline) before deletion, balancing cost and retention duration.",
      "incorrect": {
        "B": "Manual weekly deletion across 50 servers creating 1,500 objects/month is operationally inefficient, error-prone, and doesn't scale. Lifecycle policies automate this task completely.",
        "D": "Reducing backup frequency from 4 hours to daily increases the RPO from 4 hours to 24 hours, potentially losing up to 20 hours of additional game world data in a disaster scenario. Lifecycle policies enable frequent backups with cost control."
      }
    },
    "keyConceptName": "Large-Scale Backup Cost Optimization with Lifecycle Policies",
    "keyConcept": "For large-scale backup operations, use Cloud Storage Object Lifecycle Management to automatically delete old backups and transition aging backups to cheaper storage classes, enabling frequent backups with automated cost control.",
    "tags": [
      "cost-optimization",
      "object-lifecycle",
      "backup-retention",
      "storage-classes",
      "large-scale-operations"
    ],
    "examPatternKeywords": [
      "follow Google-recommended best practices",
      "Select 3",
      "1,500 backups per month",
      "reduce storage costs"
    ],
    "relatedQuestionIds": ["ace-compute-engine-045", "ace-compute-engine-047"],
    "officialDocsUrl": "https://cloud.google.com/storage/docs/lifecycle"
  },
  {
    "id": "ace-compute-engine-050",
    "domain": "compute-engine",
    "difficulty": "hard",
    "type": "multiple-select",
    "scenario": "A game studio deploys 100 dedicated servers across 5 regions on Compute Engine. Each server needs to automatically mount its 50 GB SSD persistent disk at /home/minecraft on boot, start the game server process in a detached screen session, and gracefully save world data before shutdown. The infrastructure team wants to implement this automation following Google Cloud and Linux best practices while centralizing script management for all 100 servers.",
    "question": "Which architectural patterns meet Google-recommended practices for this deployment? (Select 3)",
    "options": [
      {
        "id": "A",
        "text": "Store startup and shutdown scripts in Cloud Storage buckets for centralized version control"
      },
      {
        "id": "B",
        "text": "Configure instance metadata 'startup-script-url' and 'shutdown-script-url' pointing to the Cloud Storage scripts"
      },
      {
        "id": "C",
        "text": "Hard-code unique startup scripts on each of the 100 VMs' boot disks"
      },
      {
        "id": "D",
        "text": "Create a support ticket for Cloud Support to manually start servers after each boot"
      },
      {
        "id": "E",
        "text": "Add /etc/fstab entries on each VM to automatically mount persistent disks with appropriate options"
      }
    ],
    "correctAnswer": ["A", "B", "E"],
    "explanation": {
      "correct": "A is correct because Google recommends storing startup/shutdown scripts in Cloud Storage for centralized management, version control, and easy updates across all 100 servers without SSHing into each VM. B is correct because instance metadata keys 'startup-script-url' and 'shutdown-script-url' automatically execute scripts from Cloud Storage at boot and shutdown, providing consistent automation. E is correct because /etc/fstab entries automate persistent disk mounting at boot, which the startup script can rely on before starting the game server process.",
      "incorrect": {
        "C": "Hard-coding unique scripts on 100 VMs' boot disks creates management complexity, makes updates difficult (requires updating 100 VMs), and prevents centralized version control. Cloud Storage with metadata URLs is the scalable approach.",
        "D": "Creating support tickets for manual server starts is operationally infeasible for 100 servers and defeats the purpose of cloud automation. Metadata-driven startup scripts eliminate manual intervention."
      }
    },
    "keyConceptName": "Large-Scale VM Lifecycle Automation Architecture",
    "keyConcept": "For large-scale deployments, centralize startup/shutdown scripts in Cloud Storage, reference them via instance metadata URLs, and use /etc/fstab for disk mounting to provide consistent, automated lifecycle management across all VMs.",
    "tags": [
      "large-scale-automation",
      "instance-metadata",
      "startup-scripts",
      "centralized-management",
      "lifecycle-automation"
    ],
    "examPatternKeywords": [
      "meet Google-recommended practices",
      "Select 3",
      "100 dedicated servers",
      "centralized management"
    ],
    "relatedQuestionIds": ["ace-compute-engine-046", "ace-compute-engine-048"],
    "officialDocsUrl": "https://cloud.google.com/compute/docs/instances/startup-scripts/linux"
  }
]
